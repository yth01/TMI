{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KoGPT2_Answer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIwa4f0rU_TD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnAN6wQhV8Je",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "os.chdir(\"/content/drive/My Drive/TMI/KoGPT2\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgFiKU6VWGwX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dA3PGZ0TkUGy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "import re\n",
        "import os\n",
        "import subprocess\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import gluonnlp\n",
        "import torch\n",
        "from gluonnlp.data import SentencepieceTokenizer \n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "from kogpt2.model.sample import sample_sequence\n",
        "from kogpt2.model.torch_gpt2 import GPT2Config, GPT2LMHeadModel\n",
        "from kogpt2.utils import download, get_tokenizer, tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACdl43uY6qR0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pytorch_kogpt2 = {\n",
        "\t'url':\n",
        "\t'https://kobert.blob.core.windows.net/models/kogpt2/pytorch/pytorch_kogpt2_676e9bcfa7.params',\n",
        "\t'fname': 'pytorch_kogpt2_676e9bcfa7.params',\n",
        "\t'chksum': '676e9bcfa7'\n",
        "}\n",
        "\n",
        "kogpt2_config = {\n",
        "\t\"initializer_range\": 0.02,\n",
        "\t\"layer_norm_epsilon\": 1e-05,\n",
        "\t\"n_ctx\": 1024,\n",
        "\t\"n_embd\": 768,\n",
        "\t\"n_head\": 12,\n",
        "\t\"n_layer\": 12,\n",
        "\t\"n_positions\": 1024,\n",
        "\t\"vocab_size\": 50000,\n",
        "  \"output_past\": None\n",
        "}"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ht-bNABBqF_-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Read_Dataset(Dataset):\n",
        "\n",
        "\tdef __init__(self, file_path,vocab,tokenizer):\n",
        "\t\tself.file_path = file_path\n",
        "\t\tself.data =[]\n",
        "\t\tself.vocab =vocab\n",
        "\t\tself.tokenizer = tokenizer\n",
        "\t\tfile = open(self.file_path, 'r', encoding='utf-8')\n",
        "\n",
        "\t\tdf = pd.read_csv(self.file_path)\n",
        "\n",
        "\t\tdatasets = []\n",
        "\t\tfor _, row in df.iterrows():\n",
        "\t\t\tdatasets.append([row[\"답변\"]])\n",
        "\t\t\t\n",
        "\t\tprint(\"tokenizer ending\")\n",
        "\t\tfor line in datasets:\n",
        "\t\t\tif not line[0]:\n",
        "\t\t\t\tbreak\n",
        "\t\t\tif len(line[0]) > 1024:\n",
        "\t\t\t\tcontinue\n",
        "\t\t\telif len(line[0]) < 10:\n",
        "\t\t\t\tcontinue\n",
        "\t\t\ttoeknized_line = tokenizer(line[0][:-1])\n",
        "\n",
        "\t\t\tindex_of_words = [vocab[vocab.bos_token], ] + vocab[toeknized_line] + [vocab[vocab.eos_token]]\n",
        "\n",
        "\t\t\tif len(index_of_words) < 10:\n",
        "\t\t\t\tcontinue\n",
        "\n",
        "\t\t\tself.data.append([index_of_words])\n",
        "\n",
        "\t\tprint(np.shape(self.data))\n",
        "\n",
        "\tdef __len__(self):\n",
        "\t\treturn len(self.data)\n",
        "\n",
        "\tdef __getitem__(self, index):\n",
        "\t\titem = self.data[index]\n",
        "\t\treturn item\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dBG5n5RX7aF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def auto_enter(text):\n",
        "\ttext = (text.replace(\"   \", \"\\n\"))\n",
        "\ttext = text.split(\"\\n\")\n",
        "\n",
        "\ttext = [t.lstrip() for t in text if t != '']\n",
        "\treturn \"\\n\\n\".join(text)\n",
        "\n",
        "\n",
        "def main(epoch, save_path, load_path, samples, data_file_path, batch_size):\n",
        "\tctx = 'cuda'\n",
        "\tcachedir = '~/kogpt2/'\n",
        "\n",
        "\tsummary = SummaryWriter()\n",
        "\n",
        "\tmodel_info = pytorch_kogpt2\n",
        "\tmodel_path = download(model_info['url'],\n",
        "\t\t\t\t\t\t   model_info['fname'],\n",
        "\t\t\t\t\t\t   model_info['chksum'],\n",
        "\t\t\t\t\t\t   cachedir=cachedir)\n",
        " \n",
        "\tvocab_info = tokenizer\n",
        "\tvocab_path = download(vocab_info['url'],\n",
        "\t\t\t\t\t\t   vocab_info['fname'],\n",
        "\t\t\t\t\t\t   vocab_info['chksum'],\n",
        "\t\t\t\t\t\t   cachedir=cachedir)\n",
        "\n",
        "\tkogpt2model = GPT2LMHeadModel(config=GPT2Config.from_dict(kogpt2_config))\n",
        "\n",
        "\tkogpt2model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "\tdevice = torch.device(ctx)\n",
        "\tkogpt2model.to(device)\n",
        "\n",
        "\ttry:\n",
        "\t\tcheckpoint = torch.load(load_path, map_location=device)\n",
        "\n",
        "\t\tkogpt2model = GPT2LMHeadModel(config=GPT2Config.from_dict(kogpt2_config))\n",
        "\t\tkogpt2model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "\t\tkogpt2model.eval()\n",
        "\texcept:\n",
        "\t\tcount = 0\n",
        "\telse:\n",
        "\t\tcount = int(re.findall(\"\\d+\", load_path)[2])\n",
        "\n",
        "\tprint(count)\n",
        " \n",
        "\tkogpt2model.train()\n",
        "\tvocab_b_obj = gluonnlp.vocab.BERTVocab.from_sentencepiece(vocab_path,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t mask_token=None,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t sep_token=None,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t cls_token=None,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t unknown_token='<unk>',\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t padding_token='<pad>',\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t bos_token='<s>',\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t eos_token='</s>')\n",
        "\n",
        "\n",
        "\ttok_path = get_tokenizer()\n",
        "\tmodel, vocab = kogpt2model, vocab_b_obj\n",
        "\ttok = SentencepieceTokenizer(tok_path)\n",
        "\n",
        "\tdataset = Read_Dataset(data_file_path, vocab, tok)\n",
        "\tprint(\"Read_Dataset ok\")\n",
        "\tdata_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
        "\n",
        "\n",
        "\n",
        "\tlearning_rate = 3e-5\n",
        "\tcriterion = torch.nn.CrossEntropyLoss()\n",
        "\toptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\tprint('KoGPT-2 Transfer Learning Start')\n",
        "\tavg_loss = (0.0, 0.0)\n",
        "\n",
        "\tfor epoch in range(epoch):\n",
        "\t\tfor data in data_loader:\n",
        "\t\t\toptimizer.zero_grad()\n",
        "\t\t\tdata = torch.stack(data[0])\n",
        "\t\t\tdata = data.transpose(1,0)\n",
        "\t\t\tdata = data.to(ctx)\n",
        "\t\t\tmodel = model.to(ctx)\n",
        "\n",
        "\t\t\toutputs = model(data, labels=data)\n",
        "\t\t\tloss, logits = outputs[:2]\n",
        "\t\t\tloss = loss.to(ctx)\n",
        "\t\t\tloss.backward()\n",
        "\t\t\tavg_loss = (avg_loss[0] * 0.99 + loss, avg_loss[1] * 0.99 + 1.0)\n",
        "\t\t\toptimizer.step()\n",
        "\t\t\tif count % 10000 == 0:\n",
        "\t\t\t\tprint('epoch no.{0} train no.{1}  loss = {2:.5f} avg_loss = {3:.5f}' . format(epoch, count, loss, avg_loss[0] / avg_loss[1]))\n",
        "\t\t\t\tsummary.add_scalar('loss/avg_loss', avg_loss[0] / avg_loss[1], count)\n",
        "\t\t\t\tsummary.add_scalar('loss/loss', loss, count)\n",
        "\n",
        "\t\t\tif (count > 0 and count % 10000 == 0) or (len(data) < batch_size):\n",
        "\t\t\t\tsent = sample_sequence(model.to(\"cpu\"), tok, vocab, sent=\"저는\", text_size=100, temperature=0.7, top_p=0.8, top_k=40)\n",
        "\t\t\t\tsent = sent.replace(\"<unused0>\", \"\\n\")\n",
        "\t\t\t\tsent = auto_enter(sent)\n",
        "\t\t\t\tprint(sent)\n",
        "\n",
        "\t\t\t\tsummary.add_text('Text', sent, count)\n",
        "\n",
        "\t\t\t\tif (count > 0 and count % 100000 == 0):\n",
        "\t\t\t\t\tnow = [int(n) for n in os.listdir(samples)]\n",
        "\t\t\t\t\tnow = max(now)\n",
        "\t\t\t\t\tf = open(samples + str(now + 1), 'w', encoding=\"utf-8\")\n",
        "\t\t\t\t\tf.write(sent)\n",
        "\t\t\t\t\tf.close()\n",
        "\t\t \n",
        "\t\t\tcount += 1\n",
        "\n",
        "\t\t\tif (count > 0 and count % 10000 == 0) or (len(data) < batch_size):\n",
        "\t\t\t\t\n",
        "\t\t\t\ttry:\n",
        "\t\t\t\t\ttorch.save({\n",
        "\t\t\t\t\t\t'epoch': epoch,\n",
        "\t\t\t\t\t\t'train_no': count,\n",
        "\t\t\t\t\t\t'model_state_dict': model.state_dict(),\n",
        "\t\t\t\t\t\t'optimizer_state_dict': optimizer.state_dict(),\n",
        "\t\t\t\t\t\t'loss': loss\n",
        "\t\t\t\t\t}, save_path + 'KoGPT2_answer_checkpoint_' + str(count) + '.tar')\n",
        "\t\t\t\texcept:\n",
        "\t\t\t\t\tpass\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VbGBKFnaDwd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir(\"/content/drive/My Drive/TMI\")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Phz88YPwbITI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from preprocess import _preprocess_qna\n",
        "\n",
        "df = pd.read_csv(\"jobkorea_all.csv\")\n",
        "df = _preprocess_qna(df)\n",
        "df = df.loc[:, \"답변\"]\n",
        "df.dropna().to_csv(\"dataset.txt\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85vNCz1k8HLF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "8db454cb-48e2-4722-eacd-f765cca889a3"
      },
      "source": [
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.device_count())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKOVsTtX6caR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "d149eb47-ae91-4d8e-eef9-3b05d87c8b7d"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Aug 25 12:50:08 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.57       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P0    60W / 149W |    432MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXDB8a6pf3O0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "outputId": "a3184033-231c-4e57-87a3-d56950a8b5d1"
      },
      "source": [
        "main(epoch=30,\n",
        "     save_path=\"/content/drive/My Drive/TMI/KoGPT2/\",\n",
        "     load_path=\"/content/drive/My Drive/TMI/KoGPT2/KoGPT2_answer_checkpoint_20000.tar\",\n",
        "     samples=\"/content/drive/My Drive/TMI/KoGPT2/\",\n",
        "     data_file_path=\"dataset.txt\",\n",
        "     batch_size=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using cached model\n",
            "using cached model\n",
            "20000\n",
            "using cached model\n",
            "tokenizer ending\n",
            "(23948, 1)\n",
            "Read_Dataset ok\n",
            "KoGPT-2 Transfer Learning Start\n",
            "epoch no.0 train no.20000  loss = 4.08898 avg_loss = 4.08898\n",
            "101\n",
            "to_tokens: ['▁저는', '▁항상', '시절', '부터', '▁다양한', '께', '▁항상', '▁성실', '에게', '▁모범', '하지', '▁않고', '▁스스로', '▁노력', '▁해', '▁방법을', '▁좋아', '했습니다', '▁그래서', '▁결과', '▁저는', '▁3', '▁후', '부터', '▁지금까지', '▁부모님', '▁맡은', '▁있는', '▁일에', '▁모두', '▁스스로', '들에게', '▁의지', '하지', '▁않고', '▁스스로', '▁일을', '▁해결하는', '▁것이', '▁좋아', '했습니다', '▁이러한', '▁입학', '▁졸업', '한', '부터', '▁지금까지', '도', '▁항상', '▁항상', '들에게', '▁의지', '하지', '▁않고', '▁스스로', '▁일을', '▁하는', '하려고', '▁노력하고', '▁있습니다', '▁이러한', '▁맡은', '▁일을', '▁스스로', '▁남', '들에게', '▁의지', '하지', '▁않고', '▁스스로', '▁일을', '▁해결', '▁것이', '▁저', '▁장점', '입니다', '▁생각합니다', '▁저는', '▁저는', '▁항상', '▁남', '주', '▁스스로', '▁때', '▁남', '▁나은', '▁결과를', '▁일을', '하려고', '▁노력하고', '▁있습니다', '▁저는', '▁항상', '를', '학년', '▁때', '▁교내', '▁지금까지', '년간', '▁때까지', '▁까지', '▁교내']\n",
            "저는 대학시절부터 부모님께 항상 남들에게 의지하지 않고 스스로 일을 해결하는 것을 좋아했습니다 그 결과 대학교 입학 후부터 지금까지 제가 하고 있는 일은 모두 남들에게 의지하지 않고 스스로 일을 하는 것을 좋아합니다 대학교를 진학 후부터 지금까지도 저는 남들에게 의지하지 않고 스스로 일을 해결하려고 노력하고 있습니다 제가 좋아하는 일을 하면서 남들에게 의지하지 않고 스스로 일을 하는 것은 저의 장점이라고 생각합니다 또한 저는 항상 자기 일을 할 때 더 나은 방향으로 생각하려고 노력하고 있습니다 저는 대학교 1학년 때 부터 4학년 때 까지 교내\n",
            "epoch no.0 train no.30000  loss = 4.16576 avg_loss = 3.91530\n",
            "101\n",
            "to_tokens: ['▁저는', '▁항상', '▁2', '학년', '▁때', '▁교내', '▁학술', '▁사이트', '인', '▁연합', '▁', 's', '에서', '▁봉사활동을', '하였습니다', '▁동아리', '▁활동을', '▁중', '▁가장', '▁기억에', '▁남는', '▁활동은', '▁동아리', '▁활동', '▁중', '▁동아리', '▁활동', '▁중', '▁가장', '▁기억에', '▁남는', '▁활동은', '▁동아리', '원', '들과', '▁함께', '▁하는', '▁동아리', '대회', '였습니다', '이었습니다', '▁저는', '원', '들과', '▁함께', '▁한', '과제를', '▁하면서', '▁팀', '가', '▁원하는', '▁것을', '▁들어', '주는', '▁함께', '▁부족한', '▁대한', '▁알아', '가는', '▁시간을', '▁가', '졌습니다', '▁축구', '▁동아리', '원', '▁통해', '▁서로', '에', '▁대해', '▁알게', '▁되었고', '▁서로', '▁활동을', '▁통해', '▁서로', '▁이해할', '하는', '▁법을', '▁배', '웠습니다', '▁동아리', '▁활동을', '▁통해', '▁서로', '에', '▁대해', '▁알게', '▁되었고', '▁것은', '▁물론', '▁활동을', '▁처음', '▁난', '도', '▁마찬가지', '였습니다', '▁동아리', '▁활동을']\n",
            "저는  대학교 1학년 때 교내 커뮤니티 사이트 동아리인\n",
            "\n",
            "gfn에서 활동하였습니다 동아리 활동 중 가장 기억에 남는 활동은 동아리 활동이었습니다 동아리 활동 중 가장 기억에 남는 활동은 동아리원들과 함께 하는 축구 동아리 활동이었습니다 동아리원들과 함께 조별 과제를 하며 서로가 원하는 것을 들어주고 서로에 대해 알아가는 시간을 가졌습니다 또한 동아리 활동을 통해 서로에 대해 알게 되었고 동아리 활동을 통해 서로를 배려하는 법을 배웠습니다 동아리 활동을 통해 서로에 대해 알게 된 것은 동아리 활동이 끝나고 나서도 마찬가지였습니다 동아리 활동을\n",
            "epoch no.0 train no.40000  loss = 3.03320 avg_loss = 3.91054\n",
            "29\n",
            "to_tokens: ['▁1', '▁항상', '▁일을', '▁할', '든지', '▁항상', '▁최선을', '▁최선을', '▁다', '합니다', '▁최선을', '▁결과를', '▁얻', '▁사람', '▁되겠습니다', '▁항상', '▁최선을', '▁다', '하여', '▁좋은', '▁결과를', '▁내는', '겠습니다', '▁항상', '▁최선을', '▁사람이', '▁되기', '겠', '습', '니', '</s>']\n",
            "저는 어떤 일을 하든지 항상 항상 최선을 다하여 좋은 결과를 내는 사람이 되겠습니다 항상 최선을 다하여 좋은 결과를 내겠습니다 항상 노력하는 사람이 되겠습니</s>\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}