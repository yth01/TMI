{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KoGPT2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIwa4f0rU_TD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnAN6wQhV8Je",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "os.chdir(\"/content/drive/My Drive/TMI/KoGPT2\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgFiKU6VWGwX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dA3PGZ0TkUGy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "import re\n",
        "import subprocess\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import gluonnlp\n",
        "import torch\n",
        "from gluonnlp.data import SentencepieceTokenizer \n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "from kogpt2.model.sample import sample_sequence\n",
        "from kogpt2.model.torch_gpt2 import GPT2Config, GPT2LMHeadModel\n",
        "from kogpt2.utils import download, get_tokenizer, tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnBdTTnyCiTL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pytorch_kogpt2 = {\n",
        "\t'url':\n",
        "\t'https://kobert.blob.core.windows.net/models/kogpt2/pytorch/pytorch_kogpt2_676e9bcfa7.params',\n",
        "\t'fname': 'pytorch_kogpt2_676e9bcfa7.params',\n",
        "\t'chksum': '676e9bcfa7'\n",
        "}\n",
        "\n",
        "kogpt2_config = {\n",
        "\t\"initializer_range\": 0.02,\n",
        "\t\"layer_norm_epsilon\": 1e-05,\n",
        "\t\"n_ctx\": 1024,\n",
        "\t\"n_embd\": 768,\n",
        "\t\"n_head\": 12,\n",
        "\t\"n_layer\": 12,\n",
        "\t\"n_positions\": 1024,\n",
        "\t\"vocab_size\": 50000,\n",
        "  \"output_past\": None\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ht-bNABBqF_-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Read_Dataset(Dataset):\n",
        "\n",
        "\tdef __init__(self, file_path,vocab,tokenizer):\n",
        "\t\tself.file_path = file_path\n",
        "\t\tself.data =[]\n",
        "\t\tself.vocab =vocab\n",
        "\t\tself.tokenizer = tokenizer\n",
        "\t\tfile = open(self.file_path, 'r', encoding='utf-8')\n",
        "\n",
        "\t\tdf = pd.read_csv(self.file_path)\n",
        "\n",
        "\t\tdatasets = []\n",
        "\t\tfor _, row in df.iterrows():\n",
        "\t\t\tdatasets.append([row[\"답변\"]])\n",
        "\t\t\t\n",
        "\t\tprint(\"tokenizer ending\")\n",
        "\t\tfor line in datasets:\n",
        "\t\t\tif not line[0]:\n",
        "\t\t\t\tbreak\n",
        "\t\t\tif len(line[0]) < 3:\n",
        "\t\t\t\tcontinue\n",
        "\t\t\ttokenized_line = tokenizer(line[0][:-1])\n",
        "\n",
        "\t\t\tindex_of_words = [vocab[vocab.bos_token], ] + vocab[tokenized_line] + [vocab[vocab.eos_token]]\n",
        "\n",
        "\t\t\tif len(index_of_words) > 1024:\n",
        "\t\t\t\tcontinue\n",
        "\t\t\telif len(index_of_words) < 100:\n",
        "\t\t\t\tcontinue\n",
        "\n",
        "\t\t\tself.data.append([index_of_words])\n",
        "\n",
        "\t\tprint(np.shape(self.data))\n",
        "\n",
        "\tdef __len__(self):\n",
        "\t\treturn len(self.data)\n",
        "\n",
        "\tdef __getitem__(self, index):\n",
        "\t\titem = self.data[index]\n",
        "\t\treturn item"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dBG5n5RX7aF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def auto_enter(text):\n",
        "\ttext = (text.replace(\"   \", \"\\n\"))\n",
        "\ttext = text.split(\"\\n\")\n",
        "\n",
        "\ttext = [t.lstrip() for t in text if t != '']\n",
        "\treturn \"\\n\\n\".join(text)\n",
        "\n",
        "\n",
        "def main(epoch, save_path, load_path, samples, data_file_path, batch_size):\n",
        "\tctx = 'cuda'\n",
        "\tcachedir = '~/kogpt2/'\n",
        "\n",
        "\tsummary = SummaryWriter()\n",
        "\n",
        "\tmodel_info = pytorch_kogpt2\n",
        "\tmodel_path = download(model_info['url'],\n",
        "\t\t\t\t\t\t   model_info['fname'],\n",
        "\t\t\t\t\t\t   model_info['chksum'],\n",
        "\t\t\t\t\t\t   cachedir=cachedir)\n",
        " \n",
        "\tvocab_info = tokenizer\n",
        "\tvocab_path = download(vocab_info['url'],\n",
        "\t\t\t\t\t\t   vocab_info['fname'],\n",
        "\t\t\t\t\t\t   vocab_info['chksum'],\n",
        "\t\t\t\t\t\t   cachedir=cachedir)\n",
        "\n",
        "\tkogpt2model = GPT2LMHeadModel(config=GPT2Config.from_dict(kogpt2_config))\n",
        "\n",
        "\tkogpt2model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "\tdevice = torch.device(ctx)\n",
        "\tkogpt2model.to(device)\n",
        "\n",
        "\ttry:\n",
        "\t\tcheckpoint = torch.load(load_path, map_location=device)\n",
        "\n",
        "\t\tkogpt2model = GPT2LMHeadModel(config=GPT2Config.from_dict(kogpt2_config))\n",
        "\t\tkogpt2model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "\t\tkogpt2model.eval()\n",
        "\texcept:\n",
        "\t\tcount = 0\n",
        "\telse:\n",
        "\t\tcount = int(re.findall(\"\\d+\", load_path)[1])\n",
        "\n",
        "\tprint(count)\n",
        " \n",
        "\tkogpt2model.train()\n",
        "\tvocab_b_obj = gluonnlp.vocab.BERTVocab.from_sentencepiece(vocab_path,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t mask_token=None,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t sep_token=None,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t cls_token=None,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t unknown_token='<unk>',\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t padding_token='<pad>',\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t bos_token='<s>',\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t eos_token='</s>')\n",
        "\n",
        "\n",
        "\ttok_path = get_tokenizer()\n",
        "\tmodel, vocab = kogpt2model, vocab_b_obj\n",
        "\ttok = SentencepieceTokenizer(tok_path)\n",
        "\n",
        "\tdataset = Read_Dataset(data_file_path, vocab, tok)\n",
        "\tprint(\"Read_Dataset ok\")\n",
        "\tdata_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
        "\n",
        "\n",
        "\n",
        "\tlearning_rate = 3e-5\n",
        "\tcriterion = torch.nn.CrossEntropyLoss()\n",
        "\toptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\tprint('KoGPT-2 Transfer Learning Start')\n",
        "\tavg_loss = (0.0, 0.0)\n",
        "\n",
        "\tfor epoch in range(epoch):\n",
        "\t\tfor data in data_loader:\n",
        "\t\t\toptimizer.zero_grad()\n",
        "\t\t\tdata = torch.stack(data[0])\n",
        "\t\t\tdata = data.transpose(1,0)\n",
        "\t\t\tdata = data.to(ctx)\n",
        "\t\t\tmodel = model.to(ctx)\n",
        "\n",
        "\t\t\toutputs = model(data, labels=data)\n",
        "\t\t\tloss, logits = outputs[:2]\n",
        "\t\t\tloss = loss.to(ctx)\n",
        "\t\t\tloss.backward()\n",
        "\t\t\tavg_loss = (avg_loss[0] * 0.99 + loss, avg_loss[1] * 0.99 + 1.0)\n",
        "\t\t\toptimizer.step()\n",
        "\t\t\tif count % 100 == 0:\n",
        "\t\t\t\tprint('epoch no.{0} train no.{1}  loss = {2:.5f} avg_loss = {3:.5f}' . format(epoch, count, loss, avg_loss[0] / avg_loss[1]))\n",
        "\t\t\t\tsummary.add_scalar('loss/avg_loss', avg_loss[0] / avg_loss[1], count)\n",
        "\t\t\t\tsummary.add_scalar('loss/loss', loss, count)\n",
        "\n",
        "\n",
        "\t\t\tif (count > 0 and count % 1000 == 0) or (len(data) < batch_size):\n",
        "\t\t\t\tsent = sample_sequence(model.to(\"cpu\"), tok, vocab, sent=\"지원\", text_size=100, temperature=0.7, top_p=0.8, top_k=40)\n",
        "\t\t\t\tsent = sent.replace(\"<unused0>\", \"\\n\")\n",
        "\t\t\t\tsent = auto_enter(sent)\n",
        "\t\t\t\tprint(sent)\n",
        "\n",
        "\t\t\t\tsummary.add_text('Text', sent, count)\n",
        "\n",
        "\t\t\t\tif count > 500000:\n",
        "\t\t\t\t\tnow = [int(n) for n in os.listdir(samples)]\n",
        "\t\t\t\t\tnow = max(now)\n",
        "\t\t\t\t\tf = open(samples + str(now + 1), 'w', encoding=\"utf-8\")\n",
        "\t\t\t\t\tf.write(sent)\n",
        "\t\t\t\t\tf.close()\n",
        "\t\t \n",
        "\t\t\tcount += 1\n",
        "\n",
        "\t\t\tif (count > 0 and count % 10000 == 0) or (len(data) < batch_size):\n",
        "\t\t\t\t\n",
        "\t\t\t\ttry:\n",
        "\t\t\t\t\ttorch.save({\n",
        "\t\t\t\t\t\t'epoch': epoch,\n",
        "\t\t\t\t\t\t'train_no': count,\n",
        "\t\t\t\t\t\t'model_state_dict': model.state_dict(),\n",
        "\t\t\t\t\t\t'optimizer_state_dict': optimizer.state_dict(),\n",
        "\t\t\t\t\t\t'loss': loss\n",
        "\t\t\t\t\t}, save_path + 'KoGPT2_answer_checkpoint_' + str(count) + '.tar')\n",
        "\t\t\t\texcept:\n",
        "\t\t\t\t\tpass"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VbGBKFnaDwd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir(\"/content/drive/My Drive/TMI\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Phz88YPwbITI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from preprocess import _preprocess_qna\n",
        "\n",
        "df = pd.read_csv(\"jobkorea_all.csv\")\n",
        "df = _preprocess_qna(df)\n",
        "df = df[[\"질문\", \"답변\", \"총평\"]]\n",
        "df.loc[:, \"답변\"].to_csv(\"dataset.txt\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M__ybw6R8Ud0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "b58ab1f0-8784-41d7-cce4-e8f05a2e9cf8"
      },
      "source": [
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.device_count())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKOVsTtX6caR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "f350eedb-4ba8-493e-b6c6-ca822fe36daf"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Aug 24 15:12:59 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.57       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P0    58W / 149W |    432MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXDB8a6pf3O0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "56fd0086-adf4-4d3e-d07b-f51e5a1f6a6a"
      },
      "source": [
        "main(epoch=1,\n",
        "     save_path=\"/content/drive/My Drive/TMI/KoGPT2/\",\n",
        "     load_path=None,\n",
        "     samples=\"/content/drive/My Drive/TMI/KoGPT2/\",\n",
        "     data_file_path=\"dataset.txt\",\n",
        "     batch_size=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[██████████████████████████████████████████████████]\n",
            "[██████████████████████████████████████████████████]\n",
            "0\n",
            "using cached model\n",
            "tokenizer ending\n",
            "(23335, 1)\n",
            "Read_Dataset ok\n",
            "KoGPT-2 Transfer Learning Start\n",
            "epoch no.0 train no.0  loss = 5.63261 avg_loss = 5.63261\n",
            "epoch no.0 train no.100  loss = 5.16856 avg_loss = 4.78414\n",
            "epoch no.0 train no.200  loss = 4.04943 avg_loss = 4.67989\n",
            "epoch no.0 train no.300  loss = 4.85964 avg_loss = 4.67364\n",
            "epoch no.0 train no.400  loss = 4.88377 avg_loss = 4.64908\n",
            "epoch no.0 train no.500  loss = 4.78443 avg_loss = 4.61110\n",
            "epoch no.0 train no.600  loss = 4.92731 avg_loss = 4.62429\n",
            "epoch no.0 train no.700  loss = 4.31770 avg_loss = 4.56981\n",
            "epoch no.0 train no.800  loss = 4.55606 avg_loss = 4.56274\n",
            "epoch no.0 train no.900  loss = 4.79640 avg_loss = 4.53951\n",
            "epoch no.0 train no.1000  loss = 4.22772 avg_loss = 4.50387\n",
            "101\n",
            "to_tokens: ['▁저는', '자의', '▁지원', '학기', '▁이상의', '▁전공', '에', '▁지원', '▁다양한', '에', '▁대해', '▁전문성을', '한', '▁후', '▁지원', '개', '▁이상의', '▁직무', '에', '▁걸쳐', '▁공부', '한', '▁후', '▁2', '에', '▁따라', '▁공부', '한', '▁후', '▁직무', '개', '▁이상의', '▁직무', '에', '▁대해', '▁공부', '한', '▁후', '▁직무', '개', '▁이상의', '▁직무', '에', '▁대해', '▁공부', '한', '▁후', '▁직무', '에', '▁대해', '▁공부', '한', '▁후', '▁직무', '개', '▁이상의', '▁직무', '에', '▁대해', '▁공부', '한', '▁후', '▁직무', '개', '▁이상의', '▁직무', '에', '▁대해', '▁공부', '한', '▁후', '▁직무', '에', '▁대해', '▁공부', '한', '▁후', '▁직무', '에', '▁대해', '▁공부', '한', '▁후', '▁직무', '에', '▁대해', '▁공부', '한', '▁후', '▁직무', '에', '▁대해', '▁공부', '한', '▁후', '▁직무', '에', '▁대해', '▁공부', '한', '▁후', '▁직무', '에']\n",
            "지원자는 2개 이상의 직무에 걸쳐 직무에 대해 공부한 후 2개 이상의 직무에 대해 공부한 후 직무에 대해 공부한 후 2개 이상의 직무에 대해 공부한 후 2개 이상의 직무에 대해 공부한 후 직무에 대해 공부한 후 2개 이상의 직무에 대해 공부한 후 2개 이상의 직무에 대해 공부한 후 직무에 대해 공부한 후 직무에 대해 공부한 후 직무에 대해 공부한 후 직무에 대해 공부한 후 직무에 대해 공부한 후 직무에\n",
            "epoch no.0 train no.1100  loss = 5.05783 avg_loss = 4.46853\n",
            "epoch no.0 train no.1200  loss = 4.36868 avg_loss = 4.47096\n",
            "epoch no.0 train no.1300  loss = 4.69578 avg_loss = 4.45488\n",
            "epoch no.0 train no.1400  loss = 4.35722 avg_loss = 4.44322\n",
            "epoch no.0 train no.1500  loss = 4.61617 avg_loss = 4.43483\n",
            "epoch no.0 train no.1600  loss = 4.62796 avg_loss = 4.42099\n",
            "epoch no.0 train no.1700  loss = 4.37877 avg_loss = 4.42188\n",
            "epoch no.0 train no.1800  loss = 4.24217 avg_loss = 4.43593\n",
            "epoch no.0 train no.1900  loss = 4.31425 avg_loss = 4.41708\n",
            "epoch no.0 train no.2000  loss = 4.55424 avg_loss = 4.41870\n",
            "101\n",
            "to_tokens: ['▁제가', '자가', '▁되기', '▁싶은', '▁하는', '▁이유는', '▁가장', '▁중요하다고', '▁떠오르는', '▁것은', '입니다', '▁저는', '▁지원', '▁시절', '학년', '▁때', '부터', '▁지금까지', '▁다양한', '▁2', '학년', '때', '▁까지', '▁번도', '기를', '▁한', '▁번', '은', '▁꼭', '공', '부를', '▁했습니다', '▁하지만', '▁3', '학년', '▁때', '▁교내', '▁속한', '▁학과', '에서', '▁한', '▁학기', '▁동안', '▁한', '공', '▁듣', '▁적이', '▁있습니다', '▁저는', '▁전공', '▁듣는', '▁수업을', '▁듣는', '▁적이', '▁수업을', '을', '서를', '▁수강', '▁수행을', '▁등', '▁통해', '▁그', '▁수업', '은', '▁수업', '▁영어로', '▁후', '였습니다', '▁저는', '은', '▁대한', '▁학생들은', '▁수업', '에', '▁대한', '▁학생들은', '▁중', '▁수업', '에', '▁대한', '했습니다', '▁원', '했고', '▁저는', '▁수업', '에', '▁참여하지', '▁않았습니다', '▁수업', '▁수업', '신청', '과', '▁과제', '수행', '▁등을', '▁통해', '▁수업', '▁듣고', '▁수업을', '▁들은', '▁후', '▁수강', '신청을']\n",
            "지원자가 되고자 하는 마음이 가장 먼저 떠오르는 사람입니다 저는 고등학교 1학년때부터 지금까지 대학교 3학년 때 한 학기에 한 번씩은 학과공부를 했습니다 대학교 3학년 때는 제가 속한 과에서 한 학기 동안 학부 수업을 들은 적이 있습니다 저는 수업을 듣고 수업을 들은 후 수강신청과 과제수행 등을 했습니다 하지만 수업은 모두 끝난 뒤였습니다 수업에 참여한 후 수업에 참여한 학생 모두가 수업에 참여하기를 원했지만 저는 수업에 참여하지 않았습니다 저는 수강신청과 과제수행 등을 통해 수업을 듣고 수업을 들은 후 수강신청을\n",
            "epoch no.0 train no.2100  loss = 4.95276 avg_loss = 4.41077\n",
            "epoch no.0 train no.2200  loss = 4.15190 avg_loss = 4.39546\n",
            "epoch no.0 train no.2300  loss = 4.62999 avg_loss = 4.39857\n",
            "epoch no.0 train no.2400  loss = 4.36865 avg_loss = 4.40956\n",
            "epoch no.0 train no.2500  loss = 4.50716 avg_loss = 4.39267\n",
            "epoch no.0 train no.2600  loss = 4.83787 avg_loss = 4.37242\n",
            "epoch no.0 train no.2700  loss = 3.94660 avg_loss = 4.36855\n",
            "epoch no.0 train no.2800  loss = 4.45482 avg_loss = 4.37145\n",
            "epoch no.0 train no.2900  loss = 4.55121 avg_loss = 4.39235\n",
            "epoch no.0 train no.3000  loss = 4.21267 avg_loss = 4.40538\n",
            "101\n",
            "to_tokens: ['▁저는', '자가', '과', '에서', '▁영어', '회화', '▁수업', '사를', '▁맡', '았습니다', '▁영어', '▁영어', '▁수업', '▁중', '▁영어', '회화', '▁강', '▁맡', '었고', '▁영어', '▁것을', '▁보냈', '습니다', '▁영어', '▁중', '▁영어', '▁된', '▁수업', '은', '▁익숙', '하지', '▁않아', '▁저는', '▁영어', '회화', '▁수업', '에', '▁익숙', '하지', '수록', '▁영어', '회화', '▁대한', '▁두려', '움이', '▁커', '▁커', '졌습니다', '▁하지만', '회화', '▁수업', '가', '▁익숙', '해', '지면', '▁영어', '회화', '▁대한', '▁두려', '움이', '▁더', '지고', '▁것', '▁또한', '▁영어', '회화', '▁강의', '가', '▁함께', '▁영어', '▁맡은', '▁수업', '회화', '▁수업', '에', '▁대한', '▁자신감이', '도', '▁사라', '질', '▁것입니다', '▁저는', '▁영어', '회화', '▁강의', '에', '▁더불어', '▁제가', '회화', '▁수업', '에', '▁배운', '▁영어', '회화', '▁실', '력에', '▁통해', '▁영어', '실', '▁대한', '▁두려', '움을', '▁극복']\n",
            "지원 전공수업 중 영어회화 강사를 맡았습니다 저는 전공수업 중 영어회화 강의를 들으면서 많은 시간을 보냈습니다 그 동안 영어로 배우는 수업에 익숙하지 않았던 저는 영어회화 강의가 익숙해질수록 영어에 대한 두려움이 더욱 커졌습니다 영어회화 강의가 익숙해지면 영어에 대한 두려움은 없어질 것입니다 또한 영어회화 강의와 더불어 제가 맡은 영어회화 수업에 대한 두려움도 없어질 것입니다 저는 영어회화 강의와 더불어 영어회화 수업에서 배운 영어회화 실기를 통해 영어에 대한 두려움을 극복\n",
            "epoch no.0 train no.3100  loss = 4.58216 avg_loss = 4.38314\n",
            "epoch no.0 train no.3200  loss = 4.36817 avg_loss = 4.38142\n",
            "epoch no.0 train no.3300  loss = 5.04113 avg_loss = 4.34065\n",
            "epoch no.0 train no.3400  loss = 3.81460 avg_loss = 4.34158\n",
            "epoch no.0 train no.3500  loss = 4.28951 avg_loss = 4.36067\n",
            "epoch no.0 train no.3600  loss = 4.80776 avg_loss = 4.36233\n",
            "epoch no.0 train no.3700  loss = 3.82265 avg_loss = 4.32679\n",
            "epoch no.0 train no.3800  loss = 4.27527 avg_loss = 4.33658\n",
            "epoch no.0 train no.3900  loss = 4.11889 avg_loss = 4.33581\n",
            "epoch no.0 train no.4000  loss = 4.78621 avg_loss = 4.36172\n",
            "101\n",
            "to_tokens: ['▁저는', '동기', '▁역량', '▁생각해', '보고', '▁저는', '▁같습니다', '▁저는', '▁고등학교', '▁2', '▁다양한', '▁때까지', '학년', '▁때', '▁고등학교', '▁2', '학년', '▁때까지', '▁다양한', '▁경험을', '▁했습니다', '왔습니다', '▁고등학교', '▁때부터', '▁고등학교', '까지', '▁다양한', '▁경험을', '들을', '▁해', '▁다양한', '▁경험을', '▁쌓', '▁수', '▁있었습니다', '▁고등학교', '▁2', '▁다양한', '▁번도', '▁동안', '▁다양한', '생', '등을', '▁한', '도', '에서도', '▁장학금을', '▁받을', '▁등', '▁다양한', '▁경험을', '▁해', '왔습니다', '▁이러한', '▁고등학교', '▁진학', '▁후에도', '부터는', '▁대학교', '▁2', '▁다양한', '▁경험을', '▁해', '왔습니다', '▁다양한', '▁경험을', '▁할', '▁다양한', '▁경험을', '▁할', '▁수', '▁있었습니다', '▁이러한', '▁경험을', '들을', '▁통해', '▁다양한', '▁가진', '▁다양한', '▁더', '웠고', '▁더', '▁자신을', '▁발전', '시키고', '▁발전', '▁것을', '▁만들어', '▁많이', '시킬', '▁인재', '▁저의', '▁목표', '입니다', '▁또한', '▁이러한', '▁경험을', '▁통해', '▁다양한', '▁지식을', '▁지식을', '▁바탕으로', '▁다양한']\n",
            "지원자의 입장에서 생각해보면 다음과 같습니다 저는 중학교 때부터 고등학교 2학년 때부터 고등학교 3학년 때까지 다양한 경험을 해왔습니다 초등학교부터 고등학교까지 다양한 경험들을 통해 다양한 경험을 할 수 있었고 고등학교에서 한 학기동안 전교 1등을 하여 대학교에서도 장학금을 받는 등 다양한 경험을 해왔습니다 또한 대학 입학 후부터 대학교까지 다양한 경험을 해오면서 다양한 경험을 통해 다양한 경험을 할 수 있었습니다 이러한 경험들을 통해 제가 가진 것들을 배워서 제 자신을 발전시키고 새로운 것을 더 발전시키는 것이 제 목표입니다 또한 다양한 경험을 통해 얻은 경험과 경험을 통해 제가\n",
            "epoch no.0 train no.4100  loss = 4.42802 avg_loss = 4.34859\n",
            "epoch no.0 train no.4200  loss = 5.46651 avg_loss = 4.34392\n",
            "epoch no.0 train no.4300  loss = 4.39532 avg_loss = 4.33398\n",
            "epoch no.0 train no.4400  loss = 4.46179 avg_loss = 4.33731\n",
            "epoch no.0 train no.4500  loss = 4.37085 avg_loss = 4.31553\n",
            "epoch no.0 train no.4600  loss = 5.02652 avg_loss = 4.33765\n",
            "epoch no.0 train no.4700  loss = 4.26116 avg_loss = 4.35020\n",
            "epoch no.0 train no.4800  loss = 4.09111 avg_loss = 4.35695\n",
            "epoch no.0 train no.4900  loss = 4.03499 avg_loss = 4.32896\n",
            "epoch no.0 train no.5000  loss = 5.22309 avg_loss = 4.32803\n",
            "101\n",
            "to_tokens: ['▁저는', '동기', '과목', '▁중', '▁컴퓨터', '가지', '▁때', '▁배운', '에서', '▁배운', '과목', '인', '▁컴퓨터', '와', '에', '▁수강', '하였습니다', '▁전공', '공', '학을', '▁전공', '하였습니다', '▁전공', '▁전공', '공', '▁과목을', '▁수강', '하며', '▁컴퓨터', '공', '학을', '▁전공', '하게', '▁컴퓨터', '공학', '학을', '▁전공', '하였습니다', '▁컴퓨터', '과목', '은', '▁컴퓨터', '공학', '학을', '▁수강', '하며', '▁컴퓨터', '공학', '학을', '▁공부', '했습니다', '▁컴퓨터', '공', '학을', '▁공부', '하였습니다', '▁저는', '▁전공', '과목', '인', '▁배운', '▁컴퓨터', '▁바탕으로', '▁컴퓨터', '공', '▁과목을', '▁수강', '하며', '▁컴퓨터', '공', '에', '과목', '▁대한', '▁흥미를', '▁쌓', '았습니다', '▁또한', '▁전공', '과목', '인', '▁컴퓨터', '공', '학을', '▁수강', '하며', '▁컴퓨터', '공학', '학을', '▁공부', '하며', '▁컴퓨터', '공학', '에', '▁대한', '▁지식을', '▁쌓', '았습니다', '▁전공', '▁전공', '과목', '인', '▁컴퓨터', '공']\n",
            "지원 전공과목은 4학년 때 학과에서 전공과목인 컴퓨터공학 과목을 수강하며 컴퓨터공학을 전공했습니다 저는 컴퓨터공학 과목을 수강하며 컴퓨터공학을 전공하며 컴퓨터공학을 공부했습니다 전공과목은 컴퓨터공학을 수강하며 컴퓨터공학을 공부하며 컴퓨터공학을 전공하였습니다 제가 전공과목에서 배운 지식을 바탕으로 컴퓨터공학 과목을 수강하며 컴퓨터공학 전공에 대한 지식을 쌓았습니다 또한 전공과목인 컴퓨터공학을 수강하며 컴퓨터공학을 공부하며 컴퓨터공학에 대한 지식을 쌓았습니다 또한 전공과목인 컴퓨터공\n",
            "epoch no.0 train no.5100  loss = 4.42306 avg_loss = 4.32697\n",
            "epoch no.0 train no.5200  loss = 3.60421 avg_loss = 4.32849\n",
            "epoch no.0 train no.5300  loss = 4.49657 avg_loss = 4.31727\n",
            "epoch no.0 train no.5400  loss = 4.30171 avg_loss = 4.29360\n",
            "epoch no.0 train no.5500  loss = 3.88392 avg_loss = 4.28986\n",
            "epoch no.0 train no.5600  loss = 4.27993 avg_loss = 4.28749\n",
            "epoch no.0 train no.5700  loss = 4.09056 avg_loss = 4.29494\n",
            "epoch no.0 train no.5800  loss = 3.95077 avg_loss = 4.28456\n",
            "epoch no.0 train no.5900  loss = 4.17382 avg_loss = 4.26184\n",
            "epoch no.0 train no.6000  loss = 4.43397 avg_loss = 4.29407\n",
            "101\n",
            "to_tokens: ['▁저는', '동기', '▁저는', '▁대학', '▁시절', '▁후', '▁전공', '봉사', '활동을', '▁통해', '▁해외', '봉사를', '▁경험', '왔습니다', '▁해외', '봉사', '활동', '▁중', '▁현지', '▁해외', '원', '▁맡아', '았습니다', '▁팀', '▁팀', '장을', '▁맡아', '▁팀', '원', '들과', '▁함께', '▁해외', '봉사를', '▁다녀', '▁팀', '▁저는', '▁팀', '원', '▁맡아', '으면서', '▁팀', '원', '들과', '▁함께', '▁해외', '하는', '▁다녀', '▁것은', '▁쉽지', '▁중요하다고', '었습니다', '▁팀', '▁팀', '장을', '▁맡아', '으면서', '▁팀', '원', '들과', '▁모두가', '▁의견', '대', '로', '▁인해', '▁팀', '원', '▁모두', '▁떠', '지', '▁하는', '▁상황이', '▁발생했습니다', '▁팀', '▁팀', '장을', '▁맡', '았지만', '▁팀', '원', '▁맡', '았지만', '▁한', '▁팀', '▁팀', '원', '들과', '▁간의', '▁의견', '▁충돌이', '▁있었습니다', '▁팀', '원', '들', '▁간의', '▁갈등이', '▁생겼', '▁저는', '▁팀', '장을', '▁맡', '▁팀', '원']\n",
            "지원동기 저는 학부 졸업 후 해외봉사활동을 통해 해외봉사를 다녀왔습니다 해외봉사활동 당시 저는 팀장을 맡았습니다 저는 팀장을 맡아 팀원들과 함께 해외봉사를 했습니다 당시 저는 팀장을 맡았지만 팀원들과 함께 봉사여행을 가는 것이 가장 힘들었습니다 저는 팀장을 맡았지만 팀원들 간의 의견 충돌로 인해 팀장이 팀을 이끌어야 하는 상황이 발생했습니다 저는 팀장을 맡았지만 팀장을 맡기로 한 만큼 팀원들 간의 의견 충돌이 있었고 팀원들 간의 갈등이 있었습니다 저는 팀장을 맡아 팀원\n",
            "epoch no.0 train no.6100  loss = 4.83615 avg_loss = 4.28619\n",
            "epoch no.0 train no.6200  loss = 4.13939 avg_loss = 4.25361\n",
            "epoch no.0 train no.6300  loss = 4.37221 avg_loss = 4.26527\n",
            "epoch no.0 train no.6400  loss = 4.59597 avg_loss = 4.28962\n",
            "epoch no.0 train no.6500  loss = 4.27604 avg_loss = 4.28480\n",
            "epoch no.0 train no.6600  loss = 4.49808 avg_loss = 4.29622\n",
            "epoch no.0 train no.6700  loss = 4.23785 avg_loss = 4.27830\n",
            "epoch no.0 train no.6800  loss = 4.12315 avg_loss = 4.29887\n",
            "epoch no.0 train no.6900  loss = 4.16507 avg_loss = 4.24037\n",
            "epoch no.0 train no.7000  loss = 4.70626 avg_loss = 4.25946\n",
            "101\n",
            "to_tokens: ['▁저는', '동기', '▁저는', '▁고등학교', '▁시절', '학년', '▁때', '▁교내', '▁동아리', '실에서', '▁컴퓨터', '으로', '▁통해', '▁교내', '▁활동을', '▁했습니다', '했습니다', '▁동아리', '▁동아리', '▁활동은', '▁대한', '▁관심이', '▁가지고', '▁동아리', '▁활동을', '▁기획', '▁동아리', '▁활동을', '▁대한', '▁관심을', '▁되었습니다', '▁동아리', '▁활동을', '▁통해', '▁동아리', '▁활동을', '▁대한', '▁알게', '▁깊게', '▁수', '▁있었습니다', '▁동아리', '▁동아리', '▁활동을', '▁통해', '▁동아리', '▁활동을', '▁통해', '▁동아리', '▁활동을', '▁중요성을', '▁과정을', '▁대해', '▁알게', '▁되었고', '▁동아리', '▁활동을', '▁통해', '▁동아리', '▁활동을', '▁통해', '▁동아리', '▁활동을', '▁대한', '▁더', '▁배울', '▁알게', '▁되었습니다', '▁동아리', '▁동아리', '▁활동을', '▁통해', '▁동아리', '▁활동을', '▁통해', '▁동아리', '▁활동을', '▁통해', '▁동아리', '▁활동을', '▁통해', '▁동아리', '▁활동을', '▁통해', '▁동아리', '▁점을', '들을', '▁더', '하며', '▁동아리', '▁활동을', '▁통해', '▁동아리', '▁활동을', '▁통해', '▁얻은', '▁점을', '들을', '▁공유', '▁공유', '▁수', '▁있었습니다', '▁동아리']\n",
            "지원동기 저는 학부 1학년 때 교내 전산실 인턴을 하며 동아리 활동을 경험했습니다 당시 동아리 활동에 대해 관심을 가지고 동아리 활동을 하며 동아리 활동에 대해 알게 되었고 동아리 활동을 통해 동아리 활동에 대해 더 배울 수 있었습니다 저는 동아리 활동을 통해 동아리 활동을 하면서 동아리 활동의 전반적인 내용에 대해 알게 되었고 동아리 활동을 통해 동아리 활동을 통해 동아리 활동에 대해 더 많이 알게 되었습니다 또한 동아리 활동을 통해 동아리 활동을 통해 동아리 활동을 통해 동아리 활동을 통해 동아리 활동을 통해 느낀 점들을 공유하며 동아리 활동을 통해 동아리 활동을 통해 느낀 점들을 모두 공유할 수 있었습니다 동아리\n",
            "epoch no.0 train no.7100  loss = 4.17503 avg_loss = 4.26925\n",
            "epoch no.0 train no.7200  loss = 3.76949 avg_loss = 4.26301\n",
            "epoch no.0 train no.7300  loss = 3.81663 avg_loss = 4.27298\n",
            "epoch no.0 train no.7400  loss = 4.28397 avg_loss = 4.25753\n",
            "epoch no.0 train no.7500  loss = 3.82374 avg_loss = 4.24452\n",
            "epoch no.0 train no.7600  loss = 4.54621 avg_loss = 4.24085\n",
            "epoch no.0 train no.7700  loss = 4.01379 avg_loss = 4.27702\n",
            "epoch no.0 train no.7800  loss = 4.80559 avg_loss = 4.26281\n",
            "epoch no.0 train no.7900  loss = 4.00805 avg_loss = 4.28017\n",
            "epoch no.0 train no.8000  loss = 4.28734 avg_loss = 4.29366\n",
            "101\n",
            "to_tokens: ['▁저는', '동기', '▁가치', '▁생각해', '보', '▁저의', '은', '▁지원', '▁시절', '▁전공', '▁활동을', '▁통해', '▁동아리', '▁활동을', '▁통해', '▁진행', '했습니다', '▁일을', '▁동아리', '원', '를', '▁중요시', '▁하는', '▁팀', '리를', '▁특성상', '▁팀', '워크', '▁간의', '▁원활한', '▁의사소통', '이', '▁중요', '했고', '▁때문입니다', '▁팀', '워크', '를', '▁중요시', '했습니다', '▁되었습니다', '▁팀', '▁팀', '워크', '들과', '▁원활한', '▁의사소통', '이', '▁중요하다고', '▁것을', '▁깨달', '▁알고', '▁있었기', '▁때문에', '▁팀', '워크', '▁간의', '▁원활한', '▁의사소통', '이', '▁중요하다고', '▁중요하다고', '▁생각', '했습니다', '▁팀', '▁팀', '▁팀', '원', '▁간의', '▁함께', '▁소통을', '이', '▁중요하다고', '▁생각', '했습니다', '▁팀', '원', '▁간의', '▁함께', '▁팀', '워크', '를', '▁중요시', '했습니다', '▁팀', '▁팀', '▁활동을', '▁통해', '▁팀', '워크', '를', '▁발휘', '하여', '▁팀', '워크', '를', '▁발휘', '하는', '▁방법을', '▁배', '습', '니', '</s>']\n",
            "지원자의 입장에서 생각해 본 경험 저는 학부 시절 동아리 활동을 통해 동아리 활동을 기획하고 기획하는 과정에서 팀워크를 중요시 했습니다 동아리의 특성상 팀원 간의 원활한 의사소통이 필요하기 때문에 팀워크를 중요시하게 되었습니다 또한 팀원 간의 원활한 의사소통이 중요하다는 것을 잘 알고 있었기 때문에 팀원 간의 원활한 의사소통이 무엇보다 중요하다고 생각하였습니다 그래서 저는 팀원들과 원활한 의사소통이 중요하다고 생각하여 팀원들과 함께 팀워크를 강화했습니다 또한 동아리 활동을 통해 팀워크를 발휘하여 팀워크를 발휘하는 방법을 배웠습니</s>\n",
            "epoch no.0 train no.8100  loss = 4.16392 avg_loss = 4.25364\n",
            "epoch no.0 train no.8200  loss = 3.60453 avg_loss = 4.20591\n",
            "epoch no.0 train no.8300  loss = 3.81694 avg_loss = 4.21500\n",
            "epoch no.0 train no.8400  loss = 4.17124 avg_loss = 4.23103\n",
            "epoch no.0 train no.8500  loss = 4.49531 avg_loss = 4.22344\n",
            "epoch no.0 train no.8600  loss = 4.23167 avg_loss = 4.24190\n",
            "epoch no.0 train no.8700  loss = 4.11765 avg_loss = 4.22991\n",
            "epoch no.0 train no.8800  loss = 4.97679 avg_loss = 4.25905\n",
            "epoch no.0 train no.8900  loss = 4.21219 avg_loss = 4.20017\n",
            "epoch no.0 train no.9000  loss = 4.00718 avg_loss = 4.19739\n",
            "101\n",
            "to_tokens: ['▁저는', '동기', '▁저는', '▁대학', '교통', '에서', '▁인턴', '으로', '▁근무', '▁경험이', '▁있습니다', '▁한국', '▁한국', '으로', '▁근무하면서', '▁당시', '▁한국', '▁아르바이트', '▁경험', '하며', '▁인턴', '으로', '▁근무', '▁수행', '하면서', '▁다양한', '▁경험을', '▁경험', '했습니다', '▁인턴', '으로', '▁근무', '▁당시', '▁다양한', '▁업무를', '▁업무를', '▁경험', '했습니다', '▁인턴', '▁인턴', '▁특성상', '리와', '▁팀', '워크', '과의', '▁소통', '을', '▁통해', '▁팀', '▁수행', '하였습니다', '▁인턴', '으로', '▁근무', '▁팀', '원', '들과', '▁소통', '밀', '감을', '▁쌓', '하여', '▁팀', '▁팀', '원', '들과', '▁커뮤니케이션', '▁커뮤니케이션', '소', '통을', '▁통해', '▁업무', '▁수행', '했습니다', '▁둘째', '▁업무', '원', '들과의', '▁원활한', '▁의사', '▁통해', '▁업무', '▁진행', '했습니다', '▁팀', '으로', '▁팀', '▁수행', '하며', '▁팀', '원', '들과의', '▁원활한', '▁의사', '소', '통을', '▁통해', '▁업무를', '▁수행', '했습니다', '▁또한', '▁경험을', '은']\n",
            "지원동기 저는 한국장학재단에서 인턴으로 근무한 경험이 있습니다 저는 인턴으로 근무할 당시 다양한 업무를 경험했습니다 인턴으로 업무를 수행하며 많은 일을 경험했습니다 인턴으로 근무할 당시 저는 다양한 업무를 경험했습니다 첫째 업무처리와 팀원들과의 커뮤니케이션을 통해 업무를 진행하였습니다 인턴으로 근무하면서 팀원들과 친밀감을 형성했습니다 또한 팀원들과의 원활한 의사소통을 통해 업무를 진행했습니다 둘째 팀원들과의 원활한 소통을 통해 업무를 진행했습니다 인턴으로서 업무를 수행하며 팀원들과 원활한 의사소통을 통해 업무를 진행했습니다 이러한 경험은\n",
            "epoch no.0 train no.9100  loss = 4.21662 avg_loss = 4.20822\n",
            "epoch no.0 train no.9200  loss = 3.85938 avg_loss = 4.20590\n",
            "epoch no.0 train no.9300  loss = 4.10952 avg_loss = 4.21707\n",
            "epoch no.0 train no.9400  loss = 3.80741 avg_loss = 4.21675\n",
            "epoch no.0 train no.9500  loss = 4.09577 avg_loss = 4.22598\n",
            "epoch no.0 train no.9600  loss = 4.51546 avg_loss = 4.23559\n",
            "epoch no.0 train no.9700  loss = 4.25469 avg_loss = 4.19737\n",
            "epoch no.0 train no.9800  loss = 4.43450 avg_loss = 4.19412\n",
            "epoch no.0 train no.9900  loss = 4.25569 avg_loss = 4.21177\n",
            "epoch no.0 train no.10000  loss = 3.72305 avg_loss = 4.16102\n",
            "101\n",
            "to_tokens: ['▁저는', '동기', '▁저는', '▁고등학교', '학년', '▁동안', '▁인턴', '을', '▁하면서', '▁인턴', '원', '들과', '▁함께', '하며', '▁팀', '워크', '를', '▁발휘', '하는', '▁저는', '원', '를', '▁발휘', '하여', '▁팀', '워크', '들과', '▁간의', '▁원활한', '▁의사소통', '▁이끌어', '▁성과를', '▁달성', '해', '▁뿌듯', '▁성과를', '▁얻', '▁경험이', '▁있습니다', '▁저는', '▁팀', '원', '들과', '▁함께', '▁소통을', '으로', '▁팀', '▁통해', '▁목표를', '원', '를', '▁발휘', '하여', '▁저는', '원', '를', '▁발휘', '하여', '▁목표를', '원', '▁업무', '능력을', '▁높', '▁팀', '었습니다', '▁인턴', '▁팀', '원', '들과', '▁원활한', '▁소통을', '▁통해', '▁팀', '원들의', '를', '▁발휘', '하여', '▁좋은', '원들의', '를', '▁발휘', '하였습니다', '▁좋은', '▁성과를', '▁얻', '▁경험이', '▁있습니다', '▁저는', '▁팀', '년', '▁동안', '▁인턴', '을', '▁하며', '▁팀', '워크', '▁업무', '▁효율을', '▁높여', '주', '워크', '를', '▁발휘']\n",
            "지원동기 저는 2년 동안 인턴을 하며 팀원들과 소통하며 팀워크를 발휘했습니다 팀워크를 발휘하여 팀원들 간의 원활한 소통을 통해 목표를 달성하여 좋은 결과를 얻은 경험이 있습니다 저는 팀원들과 원활한 의사소통과 협력을 통해 팀워크를 발휘했습니다 팀워크를 발휘하여 팀원들의 업무 효율을 높여주었습니다 저는 팀원들과의 원활한 소통을 통해 팀워크를 발휘하여 팀워크를 발휘하여 좋은 결과를 얻은 경험이 있습니다 저는 2년 동안 인턴을 하며 팀원들의 업무 효율을 높여 팀워크를 발휘\n",
            "epoch no.0 train no.10100  loss = 4.03594 avg_loss = 4.23355\n",
            "epoch no.0 train no.10200  loss = 3.95041 avg_loss = 4.16472\n",
            "epoch no.0 train no.10300  loss = 4.27652 avg_loss = 4.18426\n",
            "epoch no.0 train no.10400  loss = 4.32497 avg_loss = 4.15961\n",
            "epoch no.0 train no.10500  loss = 4.41548 avg_loss = 4.20610\n",
            "epoch no.0 train no.10600  loss = 4.73578 avg_loss = 4.13593\n",
            "epoch no.0 train no.10700  loss = 3.77278 avg_loss = 4.14782\n",
            "epoch no.0 train no.10800  loss = 4.10329 avg_loss = 4.15402\n",
            "epoch no.0 train no.10900  loss = 4.27800 avg_loss = 4.15745\n",
            "epoch no.0 train no.11000  loss = 3.70712 avg_loss = 4.18312\n",
            "101\n",
            "to_tokens: ['▁저는', '동기', '▁저는', '▁전공', '생활', '▁전공', '과', '목을', '▁수강', '하며', '▁전공', '과', '목을', '▁수강', '하였습니다', '▁전공', '과', '목을', '▁수강', '하면서', '▁전공', '과', '목을', '▁수강', '하였습니다', '▁전공', '에', '목을', '▁수강', '하며', '▁전공', '과', '목을', '▁수강', '하면서', '▁전공', '과', '목을', '▁수강', '하면서', '▁전공', '과', '목을', '▁수강', '하면서', '▁전공', '과', '목을', '▁수강', '하면서', '▁전공', '과', '목을', '▁수강', '하면서', '▁전공', '과', '목을', '▁수강', '하면서', '▁전공', '과', '목을', '▁수강', '하면서', '▁전공', '과', '목을', '▁수강', '하면서', '▁전공', '과', '목을', '▁수강', '하면서', '▁전공', '과', '목을', '▁수강', '하면서', '▁전공', '과', '목을', '▁수강', '하면서', '▁전공', '과', '목을', '▁수강', '하면서', '▁전공', '과', '목을', '▁수강', '하면서', '▁전공', '과', '목을', '▁수강', '하면서', '▁전공', '과']\n",
            "지원동기 저는  대학 시절 전공과목을 수강하면서 전공과목을 수강하면서 전공과목을 수강하며 전공과목을 수강하며 전공과목을 수강하면서 전공과목을 수강하면서 전공과목을 수강하면서 전공과목을 수강하면서 전공과목을 수강하면서 전공과목을 수강하면서 전공과목을 수강하면서 전공과목을 수강하면서 전공과목을 수강하면서 전공과목을 수강하면서 전공과목을 수강하면서 전공과목을 수강하면서 전공과목을 수강하면서 전공과목을 수강하면서 전공과목을 수강하면서 전공과\n",
            "epoch no.0 train no.11100  loss = 3.86266 avg_loss = 4.16123\n",
            "epoch no.0 train no.11200  loss = 4.10362 avg_loss = 4.17909\n",
            "epoch no.0 train no.11300  loss = 4.07893 avg_loss = 4.19011\n",
            "epoch no.0 train no.11400  loss = 4.18075 avg_loss = 4.21513\n",
            "epoch no.0 train no.11500  loss = 3.82484 avg_loss = 4.18862\n",
            "epoch no.0 train no.11600  loss = 4.40276 avg_loss = 4.19727\n",
            "epoch no.0 train no.11700  loss = 3.75037 avg_loss = 4.20172\n",
            "epoch no.0 train no.11800  loss = 4.12344 avg_loss = 4.16598\n",
            "epoch no.0 train no.11900  loss = 4.87647 avg_loss = 4.16949\n",
            "epoch no.0 train no.12000  loss = 4.43126 avg_loss = 4.19648\n",
            "101\n",
            "to_tokens: ['▁저는', '동기', '▁입장에서', '▁경험', '▁통해', '▁성장', '능력', '▁저는', '▁대학교', '▁시절', '▁다양한', '▁프로젝트를', '▁통해', '▁문제해결', '능력을', '▁키', '렀', '습니다', '▁학부', '▁학부', '▁경험을', '▁통해', '▁다양한', '▁경험을', '▁통해', '▁수', '▁있었습니다', '▁저는', '▁맡은', '▁경험한', '▁한', '▁느낀', '▁것은', '▁다음과', '▁활동', '▁대한', '하였습니다', '였습니다', '▁아르바이트를', '들을', '▁쌓', '았습니다', '▁수', '▁있었습니다', '▁동아리', '▁다양한', '을', '▁통해', '▁다양한', '▁아르바이트를', '▁할', '을', '▁수', '▁있었습니다', '▁인턴', '을', '▁통해', '▁다양한', '▁경험을', '▁경험', '▁다양한', '▁경험을', '들을', '▁쌓', '을', '▁수', '▁있었습니다', '▁저는', '▁인턴', '을', '▁하면서', '▁다양한', '▁경험을', '▁할', '을', '▁수', '▁있었습니다', '▁인턴', '을', '▁통해', '▁다양한', '▁경험을', '들을', '▁쌓', '을', '▁수', '▁있었습니다', '▁인턴', '▁인턴', '리에서', '▁다양한', '▁아르바이트를', '▁할', '▁수', '▁있었습니다', '▁저는', '▁다양한', '▁활동을', '▁하면서', '▁다양한', '▁경험을', '▁할', '▁수']\n",
            "지원자의 다양한 경험을 통한 문제해결능력 저는 학부 시절 다양한 경험을 통해 문제해결능력을 길렀습니다 저는 다양한 아르바이트를 통해 다양한 경험을 할 수 있었습니다 제가 직접 아르바이트를 하면서 경험한 것은 동아리 활동에 참여하면서 다양한 경험들을 쌓을 수 있었습니다 또한 인턴십을 통해 다양한 경험을 쌓을 수 있었습니다 인턴을 하면서 다양한 아르바이트를 하면서 다양한 경험들을 쌓을 수 있었습니다 또한 인턴을 하면서 다양한 경험을 쌓을 수 있었습니다 인턴 경험을 통해 다양한 경험들을 쌓을 수 있었습니다 저는 동아리에서 다양한 경험을 할 수 있었습니다 저는 동아리 활동을 하면서 다양한 경험을 할 수\n",
            "epoch no.0 train no.12100  loss = 4.08085 avg_loss = 4.18063\n",
            "epoch no.0 train no.12200  loss = 3.64468 avg_loss = 4.18576\n",
            "epoch no.0 train no.12300  loss = 4.41580 avg_loss = 4.18665\n",
            "epoch no.0 train no.12400  loss = 3.16883 avg_loss = 4.15473\n",
            "epoch no.0 train no.12500  loss = 3.97917 avg_loss = 4.14799\n",
            "epoch no.0 train no.12600  loss = 5.00206 avg_loss = 4.14573\n",
            "epoch no.0 train no.12700  loss = 4.66715 avg_loss = 4.17544\n",
            "epoch no.0 train no.12800  loss = 4.58703 avg_loss = 4.15265\n",
            "epoch no.0 train no.12900  loss = 4.00143 avg_loss = 4.20388\n",
            "epoch no.0 train no.13000  loss = 4.55192 avg_loss = 4.16191\n",
            "101\n",
            "to_tokens: ['▁저는', '동기', '▁저는', '▁대학', '생활', '▁동안', '▁가장', '생활', '▁동안', '▁가장', '▁중요한', '▁변화를', '으로', '▁가장', '학년', '▁때', '▁학과', '▁프로젝트를', '▁수행', '▁팀', '원', '들과', '▁명이', '▁팀', '년', '▁프로젝트를', '▁진행', '▁가장', '되는', '▁팀', '▁팀', '원', '▁많은', '▁프로젝트를', '▁프로젝트를', '▁프로젝트를', '▁진행', '해야', '▁했습니다', '▁팀', '원', '▁한', '▁모두', '▁프로젝트', '▁상황에서', '▁속에서', '▁팀', '원', '▁한', '▁간의', '▁의견', '▁있었고', '▁저는', '▁팀', '원', '들', '▁간의', '▁갈등이', '에', '▁과정에서', '▁팀', '원', '들', '▁간의', '▁의견', '▁가장', '▁큰', '▁문제', '이라고', '▁생각', '했습니다', '▁그래서', '원', '▁갈등이', '▁해소', '하여', '▁팀', '에서', '▁지연', '되는', '▁경우가', '▁가장', '▁큰', '▁문제', '였습니다', '▁저는', '원', '들', '▁간의', '▁갈등이', '▁발생', '하여', '▁팀', '원들이', '들', '▁간의', '▁갈등이', '▁발생', '▁경우', '▁팀', '원', '들', '▁갈등이']\n",
            "지원동기 저는 대학생활 중 대학생활에서 가장 큰 어려움으로 1학년 때 팀 프로젝트를 진행하면서 팀원 한 명이 2년간 프로젝트에서 제외되어 다른 팀보다 늦게까지 남아 프로젝트를 진행해야 했습니다 팀원들 모두 그 상황 속에서 팀원들 간의 갈등이 있었습니다 저는 팀원들 간의 갈등 해결 과정에서 팀원들 간의 갈등이 가장 큰 원인이라고 생각했습니다 팀원들의 갈등이 발생하여 프로젝트 진행이 지연되는 것이 가장 큰 문제였습니다 팀원들 간의 갈등이 발생하여 팀원들 간의 갈등이 발생할 경우 팀원 간의 갈등이\n",
            "epoch no.0 train no.13100  loss = 4.48692 avg_loss = 4.17978\n",
            "epoch no.0 train no.13200  loss = 4.16308 avg_loss = 4.14509\n",
            "epoch no.0 train no.13300  loss = 4.00342 avg_loss = 4.15586\n",
            "epoch no.0 train no.13400  loss = 3.81517 avg_loss = 4.18687\n",
            "epoch no.0 train no.13500  loss = 3.71293 avg_loss = 4.16043\n",
            "epoch no.0 train no.13600  loss = 4.02353 avg_loss = 4.13376\n",
            "epoch no.0 train no.13700  loss = 3.86548 avg_loss = 4.13545\n",
            "epoch no.0 train no.13800  loss = 4.18412 avg_loss = 4.14293\n",
            "epoch no.0 train no.13900  loss = 3.42216 avg_loss = 4.13472\n",
            "epoch no.0 train no.14000  loss = 4.28948 avg_loss = 4.16132\n",
            "101\n",
            "to_tokens: ['▁저는', '동기', '기는', '▁중요성', '함', '▁저는', '▁대학', '▁적', '부터', '▁부모', '님의', '▁영향으로', '핌', '을', '▁받으며', '▁부모', '님의', '▁항상', '▁저를', '에게', '▁항상', '▁것을', '▁가르쳐', '▁주', '셨습니다', '▁부모', '▁저는', '▁부모', '▁부모', '과', '▁함께', '▁지내', '왔습니다', '▁부모', '▁덕분에', '▁항상', '▁부모', '님의', '▁생각하며', '▁살아', '왔습니다', '▁부모', '님의', '▁보살', '핌', '을', '▁받으며', '▁자라', '온', '▁때문에', '▁부모', '▁부모', '님의', '▁보살', '▁많이', '왔습니다', '▁또한', '님의', '▁보살', '▁없이는', '▁저', '▁저', '게', '▁가장', '▁큰', '▁것은', '▁부모', '과', '▁함께', '▁살아', '하며', '▁것입니다', '▁저는', '님의', '▁보살', '은', '▁저', '게', '▁가장', '▁힘이', '양', '분이', '▁되었고', '▁저', '▁성장할', '▁데', '▁큰', '▁원동', '▁되었습니다', '▁부모', '▁후', '▁아니라', '▁저는', '님의', '▁가르침', '은', '▁저', '게', '▁큰', '▁자', '▁되었습니다', '▁부모', '과']\n",
            "지원 동기의 간절함 저는 어릴 적부터 부모님의 보살핌을 받았습니다 부모님은 항상 저에게 많은 것을 가르쳐 주셨습니다 특히 저는 항상 부모님과 함께 살아왔고 그 때문에 항상 부모님을 생각하며 살아왔습니다 부모님의 보살핌을 받으며 자라왔기 때문에 항상 부모님의 영향을 받아왔습니다 부모님의 도움은 물론 제게 가장 소중한 것은 부모님과 함께 생활한다는 것입니다 부모님의 가르침은 제게 큰 자양분이 되었고 제가 성장하는 데 큰 도움이 되었습니다 그뿐만 아니라 부모님의 가르침은 제게 큰 힘이 되었습니다 부모님과\n",
            "epoch no.0 train no.14100  loss = 3.86092 avg_loss = 4.11539\n",
            "epoch no.0 train no.14200  loss = 4.20677 avg_loss = 4.15499\n",
            "epoch no.0 train no.14300  loss = 4.13383 avg_loss = 4.12212\n",
            "epoch no.0 train no.14400  loss = 4.09430 avg_loss = 4.10717\n",
            "epoch no.0 train no.14500  loss = 4.61750 avg_loss = 4.14050\n",
            "epoch no.0 train no.14600  loss = 4.04266 avg_loss = 4.10286\n",
            "epoch no.0 train no.14700  loss = 3.87351 avg_loss = 4.05614\n",
            "epoch no.0 train no.14800  loss = 4.17896 avg_loss = 4.06479\n",
            "epoch no.0 train no.14900  loss = 4.60149 avg_loss = 4.09982\n",
            "epoch no.0 train no.15000  loss = 5.35747 avg_loss = 4.12399\n",
            "101\n",
            "to_tokens: ['▁저는', '동기', '▁저는', '▁대학', '▁3', '학년', '▁때', '▁교내', '공', '학과를', '▁전공', '하며', '▁컴퓨터', '에', '▁전공', '했습니다', '▁프로그래밍', '에', '▁대한', '▁느꼈', '습니다', '▁하지만', '▁후', '▁프로그래밍', '을', '▁전공', '하며', '▁프로그래밍', '에', '▁대한', '▁흥미를', '▁느꼈', '고', '▁프로그래밍', '과', '식을', '▁쌓', '왔습니다', '갔습니다', '▁또한', '▁컴퓨터', '공학', '학을', '▁전공', '하며', '▁프로그래밍', '에', '▁대한', '▁흥미를', '▁느꼈', '습니다', '▁전공', '지', '식을', '▁바탕으로', '기', '▁위해', '▁프로그래밍', '▁프로그래밍', '▁언어를', '와', '▁언어', '▁공부', '하며', '▁프로그래밍', '▁중', '▁가장', 'ot', '를', '▁대한', '▁관심을', '▁했습니다', '▁프로그래밍', 'ot', '에', '▁대한', '▁배', '▁느꼈', '습니다', '▁또한', '▁i', 'ot', '에', '▁대한', '▁공부를', '▁쌓', '기', '▁위해', '▁i', 'i', 'mula', 'te', '을', '▁대한', '▁공부', '했습니다', '▁i', '▁중', '▁i', 'ot', '에', '▁기본', '에']\n",
            "지원동기 저는 대학교 2학년 때 컴퓨터공학을 전공하며 프로그래밍을 전공하며 프로그래밍에 흥미를 느꼈습니다 그 후 프로그래밍을 전공하며 프로그래밍에 대한 흥미를 느꼈고 전공지식을 쌓아나갔습니다 또한 컴퓨터공학을 전공하며 프로그래밍에 대한 흥미를 느꼈습니다 전공지식을 쌓기 위해 다양한 프로그래밍 언어 및 언어를 공부했고 그 중 iot에 대한 공부를 하면서 iot에 대해 흥미를 느꼈습니다 또한 iot에 대한 지식을 쌓기 위해 simulation에 대해 공부했고 그 중 iot의 활용과\n",
            "epoch no.0 train no.15100  loss = 4.01243 avg_loss = 4.11027\n",
            "epoch no.0 train no.15200  loss = 4.42543 avg_loss = 4.11296\n",
            "epoch no.0 train no.15300  loss = 3.86384 avg_loss = 4.10636\n",
            "epoch no.0 train no.15400  loss = 4.89041 avg_loss = 4.14772\n",
            "epoch no.0 train no.15500  loss = 4.97238 avg_loss = 4.15096\n",
            "epoch no.0 train no.15600  loss = 4.58485 avg_loss = 4.07903\n",
            "epoch no.0 train no.15700  loss = 4.22362 avg_loss = 4.14933\n",
            "epoch no.0 train no.15800  loss = 4.40368 avg_loss = 4.14262\n",
            "epoch no.0 train no.15900  loss = 3.93595 avg_loss = 4.12428\n",
            "epoch no.0 train no.16000  loss = 3.95276 avg_loss = 4.12588\n",
            "101\n",
            "to_tokens: ['▁저는', '동기', '▁저는', '년', '▁인턴', 'fa', 'r', '센터에서', '▁c', '으로', '원으로', '▁근무', '하며', '▁c', 's', '교육', '▁통해', '▁c', '과의', '▁마인', '프로', '▁익', '혔습니다', '▁c', '▁통해', '▁c', '지향', '적인', '▁사고를', '▁프로세', '▁익', '히고', '▁또한', '▁통해', '▁고객', 's', '의', '센터에서', '▁고객', '지향', '적', '▁업무', '▁프로세스를', '▁익', '혔습니다', '▁수', '▁있었습니다', '▁또한', '년간', '▁전', '▁c', 's', '교육', '센터에서', '▁인턴', '사', '원으로', '▁근무', '하며', '▁고객', 's', '교육', '센터에서', '▁교육', '프로그램을', '▁대한', '▁이해', '▁이해와', '▁실무', '▁키', '았습니다', '▁c', '▁c', 's', '교육', '센터에서', '▁교육', '프로그램을', '▁개발', '▁교육', '▁프로그램을', '▁전반적인', '▁및', '▁교육', '▁담당', '했습니다', '▁교육', '▁프로그램의', '▁대한', '▁전반적인', '▁지식을', '▁쌓', '았습니다', '▁3', '년', '▁간', '▁c', 's', '교육', '센터에서', '▁교육', '사', '원으로']\n",
            "지원동기 1년간 cs교육센터에서 인턴사원으로 근무하며 cs교육을 통해 고객중심의 업무 프로세스를 익혔습니다 이를 통해 고객지향적 업무 프로세스를 익혔습니다 이를 통해 cs교육센터의 고객중심적인 업무 프로세스를 익힐 수 있었습니다 2년 간 cs교육센터에서 인턴사원으로 근무하며 cs교육 및 교육 프로그램에 대한 전반적인 이해와 역량을 쌓았습니다 또한 cs교육센터에서 교육 프로그램을 활용하여 교육 프로그램의 운영 및 관리를 담당하며 교육 프로그램에 대한 전반적인 지식을 쌓았습니다 3년 간 cs교육센터에서 인턴사원으로\n",
            "epoch no.0 train no.16100  loss = 3.78181 avg_loss = 4.16089\n",
            "epoch no.0 train no.16200  loss = 4.67405 avg_loss = 4.11480\n",
            "epoch no.0 train no.16300  loss = 4.33212 avg_loss = 4.13515\n",
            "epoch no.0 train no.16400  loss = 3.74849 avg_loss = 4.13453\n",
            "epoch no.0 train no.16500  loss = 4.33287 avg_loss = 4.14625\n",
            "epoch no.0 train no.16600  loss = 4.14956 avg_loss = 4.15760\n",
            "epoch no.0 train no.16700  loss = 3.97800 avg_loss = 4.06204\n",
            "epoch no.0 train no.16800  loss = 4.56744 avg_loss = 4.09245\n",
            "epoch no.0 train no.16900  loss = 4.41123 avg_loss = 4.08240\n",
            "epoch no.0 train no.17000  loss = 4.43552 avg_loss = 4.10312\n",
            "101\n",
            "to_tokens: ['▁저는', '동기', '▁저는', '▁후', '▁제가', '년간', '▁동안은', '▁인턴', '에', '▁자격증', '은', '▁많지만', '▁다양한', '▁관련', '▁지식을', '▁취득', '을', '▁통해', '▁직무', '에', '▁대한', '▁전문성을', '▁갖추', '하겠습니다', '▁수', '▁있었습니다', '▁노력', '하겠습니다', '▁또한', '▁후', '▁1', '년', '▁동안은', '▁직무', '▁관련', '▁자격증', '▁취득', '을', '▁통해', '▁직무', '▁관련', '▁역량을', '▁강화', '겠습니다', '▁입사', '▁후', '▁1', '년', '▁동안은', '▁직무', '▁관련', '▁자격증', '▁취득', '을', '▁통해', '▁직무', '▁관련', '▁역량을', '▁향상', '하겠습니다', '▁입사', '▁후', '▁5', '년', '▁후에는', '▁직무', '▁관련', '▁전문성을', '▁취득', '을', '▁통해', '▁직무', '▁관련', '▁역량을', '▁향상', '하겠습니다', '▁입사', '▁후', '▁10', '년', '▁동안은', '▁직무', '▁관련', '▁자격증', '▁취득', '을', '▁통해', '▁직무', '▁관련', '▁역량을', '▁향상', '할', '▁입사', '▁후', '년', '▁후에는', '▁직무', '▁관련', '▁자격증', '▁취득', '을', '▁통해', '▁직무']\n",
            "지원동기 입사 후 1년 동안 직무 관련 경험은 물론 직무 관련 자격증 취득을 통해 직무에 대한 전문성을 향상할 수 있도록 노력하겠습니다 입사 후 2년 동안은 직무 관련 자격증 취득을 통해 직무 관련 역량을 쌓겠습니다 입사 후 5년 동안은 직무 관련 자격증 취득을 통해 직무 관련 역량을 향상하겠습니다 입사 후 10년 동안은 직무 관련 자격증 취득을 통해 직무 관련 전문성을 향상하겠습니다 입사 후 10년 동안은 직무 관련 자격증 취득을 통해 직무 관련 역량을 향상하겠습니다 입사 10년 동안은 직무 관련 자격증 취득을 통해 직무\n",
            "epoch no.0 train no.17100  loss = 3.82810 avg_loss = 4.10592\n",
            "epoch no.0 train no.17200  loss = 4.28904 avg_loss = 4.09088\n",
            "epoch no.0 train no.17300  loss = 4.16391 avg_loss = 4.07684\n",
            "epoch no.0 train no.17400  loss = 4.53046 avg_loss = 4.12579\n",
            "epoch no.0 train no.17500  loss = 4.18023 avg_loss = 4.05873\n",
            "epoch no.0 train no.17600  loss = 3.99542 avg_loss = 4.08651\n",
            "epoch no.0 train no.17700  loss = 3.70054 avg_loss = 4.08154\n",
            "epoch no.0 train no.17800  loss = 4.10143 avg_loss = 4.08719\n",
            "epoch no.0 train no.17900  loss = 4.18157 avg_loss = 4.07219\n",
            "epoch no.0 train no.18000  loss = 3.86565 avg_loss = 4.07394\n",
            "101\n",
            "to_tokens: ['▁저는', '동기', '▁저는', '▁대학', '▁2', '학년', '▁때', '▁교내', '기사', '사', '▁자격증을', '급', '▁자격증을', '시험', '에서', '▁1', '등을', '▁한', '▁전기', '기능', '사', '▁1', '급을', '▁자격증을', '▁합격', '하였습니다', '▁저는', '▁저는', '학년', '▁때', '▁실기', '기능', '사', '▁2', '급', '▁실기', '시험', '에서', '▁1', '등을', '▁하여', '▁2', '하지', '▁하지만', '▁1', '▁3', '학년', '▁때', '▁전기', '기능', '사', '▁1', '급', '▁실기', '시험', '에서', '▁1', '등을', '▁하여', '▁전기', '하였습니다', '▁하지만', '▁2', '학년', '▁때는', '▁전기', '기능', '사', '▁1', '급', '▁실기', '시험', '에서', '▁1', '등을', '▁하여', '▁합격', '하였습니다', '▁3', '학년', '▁때는', '▁전기', '기능', '사', '▁1', '급', '▁실기', '시험', '에서', '▁1', '등을', '▁하여', '▁합격', '하였습니다', '▁저는', '▁대학교', '▁3', '학년', '▁때', '▁전기', '기능', '사', '▁1']\n",
            "지원동기 저는 대학교 3학년 때 전기기능사 1급 실기시험에서 1등을 하여 전기기능사 1급에 합격하였습니다 하지만 2학년 때는 전기기능사 1급 실기시험에서 1등을 하여 합격했습니다 저는 대학교 3학년 때 전기기능사 1급 실기시험에서 1등을 하여 합격하였습니다 하지만 2학년 때는 전기기능사 1급 실기시험에서 1등을 하여 합격하였고 3학년 때는 전기기능사 1급 실기시험에서 1등을 하여 합격하였습니다 저는 고등학교 3학년 때 전기기능사 1\n",
            "epoch no.0 train no.18100  loss = 4.12380 avg_loss = 4.06271\n",
            "epoch no.0 train no.18200  loss = 4.41237 avg_loss = 4.11210\n",
            "epoch no.0 train no.18300  loss = 4.45237 avg_loss = 4.07296\n",
            "epoch no.0 train no.18400  loss = 3.75014 avg_loss = 4.03889\n",
            "epoch no.0 train no.18500  loss = 3.79605 avg_loss = 4.06315\n",
            "epoch no.0 train no.18600  loss = 4.24065 avg_loss = 4.05357\n",
            "epoch no.0 train no.18700  loss = 4.61835 avg_loss = 4.10720\n",
            "epoch no.0 train no.18800  loss = 3.70315 avg_loss = 4.11594\n",
            "epoch no.0 train no.18900  loss = 4.52610 avg_loss = 4.09832\n",
            "epoch no.0 train no.19000  loss = 3.86077 avg_loss = 4.11068\n",
            "101\n",
            "to_tokens: ['▁저는', '동기', '▁저는', '▁대학', '▁경험을', '경험을', '▁통해', '▁다양한', '▁가진', '▁역량을', '▁발휘', '보고', '▁경험이', '▁있습니다', '▁대학', '시절', '▁다양한', '▁사회', '경험을', '과', '▁다양한', '▁다양한', '▁대외', '대의', '▁사람들과', '▁어울', '리며', '▁다양한', '▁경험을', '경험을', '▁할', '을', '▁특히', '▁중', '▁저는', '▁다양한', '▁3', '학년', '▁때', '▁교내', '학생', '으로', '▁교환', '로', '▁교환', '학생', '을', '▁다녀', '왔습니다', '▁교환', '▁당시', '▁저는', '▁교환', '▁사회', '▁온', '학생', '▁생활을', '▁하며', '▁다양한', '▁문화를', '▁언어를', '▁접할', '▁다양한', '▁경험을', '▁어울', '할', '▁교류', '하며', '▁다양한', '▁문화를', '▁쌓', '▁수', '▁있었습니다', '▁또한', '▁결과', '▁아니라', '▁저는', '▁다양한', '▁국적의', '경험을', '▁통해', '▁다양한', '▁사람들과', '권의', '▁사람들과', '▁교류', '하며', '▁다양한', '▁경험을', '▁교류', '하고', '▁소통', '하며', '▁다양한', '▁경험을', '▁할', '을', '▁수', '▁있었습니다', '▁이러한', '▁저는', '▁다양한', '▁사회']\n",
            "지원동기 저는 다양한 사회경험을 통해 제가 가진 역량을 발휘해본 경험이 있습니다 대학시절 다양한 사회경험과 함께 다양한 연령대의 사람들과 어울리며 다양한 사회경험을 쌓았습니다 그 결과 저는 대학교 1학년 때 교환학생으로 프랑스로 교환학생을 다녀왔습니다 그 결과 저는 여러 나라에서 교환학생 생활을 하면서 다양한 문화와 언어를 접하며 다양한 사람들과 교류하고 소통하며 다양한 경험을 할 수 있었습니다 그뿐만 아니라 저는 다양한 사회경험을 통해 다양한 문화권의 사람들과 교류하며 다양한 사람들과 교류하고 소통하며 다양한 경험을 쌓을 수 있었습니다 또한 저는 다양한 연령\n",
            "epoch no.0 train no.19100  loss = 4.32060 avg_loss = 4.07227\n",
            "epoch no.0 train no.19200  loss = 4.26688 avg_loss = 4.10362\n",
            "epoch no.0 train no.19300  loss = 4.12713 avg_loss = 4.10814\n",
            "epoch no.0 train no.19400  loss = 4.58587 avg_loss = 4.13840\n",
            "epoch no.0 train no.19500  loss = 3.77316 avg_loss = 4.07953\n",
            "epoch no.0 train no.19600  loss = 4.22713 avg_loss = 4.05878\n",
            "epoch no.0 train no.19700  loss = 4.19117 avg_loss = 4.07196\n",
            "epoch no.0 train no.19800  loss = 4.05195 avg_loss = 4.05818\n",
            "epoch no.0 train no.19900  loss = 4.16420 avg_loss = 4.06304\n",
            "epoch no.0 train no.20000  loss = 4.59836 avg_loss = 4.06189\n",
            "101\n",
            "to_tokens: ['▁저는', '동기', '▁저는', '▁대학', '▁4', '학년', '▁때', '▁교내', '활동', '▁동아리', '활동', '에서', '▁팀', '원', '들과', '▁함께', '▁과제를', '▁진행', '하였습니다', '▁조별', '▁번째', '▁팀', '원', '들과', '▁함께', '▁프로젝트를', '▁진행', '했습니다', '▁프로젝트', '▁주제는', '▁팀', '▁기획', '▁시', '▁조별', '▁진행', '▁상황을', '▁및', '▁발생할', '해야', '▁할', '▁사항을', '였습니다', '▁정리한', '▁것입니다', '▁이', '▁중', '▁팀', '원', '들과', '▁의견', '▁조율', '하고', '▁진행', '▁진행', '에', '▁필요한', '해야', '▁중', '▁수행', '해야', '▁할', '▁과제', '들을', '▁정리', '했습니다', '▁정리', '원', '들에게', '▁전달', '하였습니다', '▁그', '▁결과', '▁프로젝트', '원', '▁프로젝트', '▁수행', '▁중', '▁있어', '▁가장', '▁중요한', '▁역할을', '▁하는', '▁칭찬', '했습니다', '았습니다', '▁두', '▁경험을', '은', '▁제가', '원', '들과', '▁함께', '▁프로젝트를', '▁진행하는', '▁때', '▁가장', '▁역할을', '▁할', '▁것입니다', '▁있다고', '▁것입니다', '▁생각', '합']\n",
            "지원동기 저는 대학교 2학년 때 대외활동 조별 과제에서 팀원들과 함께 프로젝트를 기획했습니다 첫 번째로 팀원들과 함께 프로젝트를 진행했습니다 프로젝트의 주제는 프로젝트 수행 전 프로젝트 진행 과정 중 수행해야 할 과제들을 정리한 것입니다 그 과정에서 팀원들과 의견을 조율하고 프로젝트 진행 시 진행 과정 중 수행해야 할 과제들을 정리하여 팀원들에게 제출했습니다 그 결과 팀원들은 프로젝트 진행에 있어 가장 중요한 역할을 했다고 평가받았습니다 이 경험은 팀원들과 함께 프로젝트를 진행할 때 중요한 역할을 할 수 있을 것이라고 생각합\n",
            "epoch no.0 train no.20100  loss = 3.85106 avg_loss = 4.02968\n",
            "epoch no.0 train no.20200  loss = 3.31214 avg_loss = 4.06227\n",
            "epoch no.0 train no.20300  loss = 3.74426 avg_loss = 3.99788\n",
            "epoch no.0 train no.20400  loss = 3.44515 avg_loss = 4.08272\n",
            "epoch no.0 train no.20500  loss = 3.94329 avg_loss = 4.08146\n",
            "epoch no.0 train no.20600  loss = 4.03103 avg_loss = 4.09558\n",
            "epoch no.0 train no.20700  loss = 3.68002 avg_loss = 4.07681\n",
            "epoch no.0 train no.20800  loss = 4.22591 avg_loss = 4.08243\n",
            "epoch no.0 train no.20900  loss = 3.21384 avg_loss = 4.02055\n",
            "epoch no.0 train no.21000  loss = 4.34237 avg_loss = 4.03504\n",
            "101\n",
            "to_tokens: ['▁저는', '동기', '▁저는', '▁배운', '▁대학', '▁사람', '과', '자들과', '▁함께', '▁융합', '적', '▁인재', '입니다', '▁저는', '▁전공', '과목', '▁바탕으로', '▁전공', '자들', '들로', '▁저는', '▁다른', '기능', '사', '▁자격증', '▁취득', '▁도전', '하여', '▁전기', '기능', '장', '▁자격증을', '급', '▁취득', '하였습니다', '▁전기', '기능', '장', '▁자격증', '▁취득', '하기', '▁위해', '▁전기', '기능', '장', '▁자격증', '▁취득', '하였습니다', '▁전기', '▁후', '▁전기', '기능', '장', '▁자격증', '▁준비', '하면서', '▁전기', '기능', '장', '▁자격증', '▁취득', '▁도전', '하였습니다', '▁전기', '▁과정에서', '▁저는', '▁전기', '▁전공', '자들과', '의', '▁융합', '형', '▁인재', '로서', '▁전기', '▁전기', '▁전기', '기능', '장', '▁자격증', '▁취득', '하기', '▁당시', '▁전기', '기능', '장', '▁자격증', '에', '▁도전', '하여', '▁합격', '기능', '장', '▁자격증을', '급을', '▁취득', '하였습니다', '▁수', '▁있었습니다', '▁저는', '▁저는', '▁전기', '▁전공']\n",
            "지원동기에서 저는 다른 전공 전공자들과의 융합형 인재입니다 다양한 전공 지식을 가진 전공자 중에서도 저는 전기기능장 자격증에 도전하여 전기기능사 1급을 취득했습니다 전기기능장 자격증을 취득하기 위해 전기기능장 자격증을 취득하였습니다 그 후 전기기능장 시험을 준비하며 전기기능장 자격증에 도전하였습니다 이 과정에서 저는 다른 전공자들과의 융합형 인재였습니다 특히 저는 전기기능장 자격증을 취득할 당시 전기기능장 자격증에 도전하여 전기기능장 1급을 취득할 수 있었습니다 하지만 저는 다른 전공\n",
            "epoch no.0 train no.21100  loss = 3.68090 avg_loss = 4.04614\n",
            "epoch no.0 train no.21200  loss = 3.66520 avg_loss = 4.04312\n",
            "epoch no.0 train no.21300  loss = 4.23647 avg_loss = 4.06506\n",
            "epoch no.0 train no.21400  loss = 4.37795 avg_loss = 4.10183\n",
            "epoch no.0 train no.21500  loss = 4.48197 avg_loss = 4.07542\n",
            "epoch no.0 train no.21600  loss = 4.09484 avg_loss = 4.06534\n",
            "epoch no.0 train no.21700  loss = 4.11115 avg_loss = 4.04086\n",
            "epoch no.0 train no.21800  loss = 3.99782 avg_loss = 4.10820\n",
            "epoch no.0 train no.21900  loss = 4.21767 avg_loss = 4.06220\n",
            "epoch no.0 train no.22000  loss = 4.50842 avg_loss = 4.08139\n",
            "101\n",
            "to_tokens: ['▁저는', '동기', '▁저는', '▁대학', '과목', '▁및', '▁다양한', '▁경험을', '들을', '▁다양한', '에', '▁갖추', '았습니다', '▁수', '▁있었습니다', '▁특히', '학년', '▁때', '▁교내', '▁전산', '회계', '▁인턴', '실', '▁회계', '실', '▁업무를', '인턴', '으로', '▁근무', '하였습니다', '▁전산', '실', '▁행정', '▁전산', '실', '▁업무를', '▁전산', '실', '▁내', '보조', '▁업무를', '▁수행', '했습니다', '▁전산', '년', '▁동안', '▁전산', '업무를', '▁수행', '하며', '▁전산', '실', '▁내', '▁전산', '실', '▁내', '▁전산', '실', '▁내', '▁보조', '하였습니다', '▁업무를', '▁수행', '하였습니다', '▁2', '실', '▁내', '▁전산', '실', '▁내', '▁보조', '하는', '▁전산', '처리', '에', '▁업무', '실', '▁내', '▁전산', '실', '▁내', '▁수행', '▁능력을', '▁향상', '시켰습니다', '▁수', '▁있었습니다', '▁2', '학년', '▁때', '▁교내', '▁전산', '실', '▁행정', '인턴', '으로', '▁근무', '하며', '▁전산', '실', '▁내', '▁전산']\n",
            "지원동기 저는 전공지식 외에도 다양한 경험으로 실무능력을 쌓을 수 있었습니다 2학년 때  교내 전산실 행정실에서 전산실 행정인턴으로 근무하며 전산실 내 전산실 내 전산실 업무 보조 업무를 수행하였습니다 1년 동안 보조업무를 수행하며 전산실 내 전산실 내 전산실 업무를 보조하는 업무를 수행하였습니다 전산실 내 전산실 업무를 보조하면서 업무수행 능력과 전산실 내 전산실 업무 수행 능력을 향상할 수 있었습니다 2학년 때 교내 전산실 행정인턴으로 근무하며 전산실 내 전산\n",
            "epoch no.0 train no.22100  loss = 3.80771 avg_loss = 4.09319\n",
            "epoch no.0 train no.22200  loss = 3.71406 avg_loss = 4.11093\n",
            "epoch no.0 train no.22300  loss = 3.62483 avg_loss = 4.05942\n",
            "epoch no.0 train no.22400  loss = 4.44533 avg_loss = 4.08373\n",
            "epoch no.0 train no.22500  loss = 4.40997 avg_loss = 4.10441\n",
            "epoch no.0 train no.22600  loss = 4.31582 avg_loss = 4.07802\n",
            "epoch no.0 train no.22700  loss = 3.90867 avg_loss = 4.06819\n",
            "epoch no.0 train no.22800  loss = 3.97825 avg_loss = 4.06225\n",
            "epoch no.0 train no.22900  loss = 4.62566 avg_loss = 4.05586\n",
            "epoch no.0 train no.23000  loss = 4.66567 avg_loss = 4.08148\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
