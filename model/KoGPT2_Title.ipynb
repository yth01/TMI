{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KoGPT2_Title.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIwa4f0rU_TD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnAN6wQhV8Je",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "os.chdir(\"/content/drive/My Drive/TMI/KoGPT2\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgFiKU6VWGwX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dA3PGZ0TkUGy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "import re\n",
        "import subprocess\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import gluonnlp\n",
        "import torch\n",
        "from gluonnlp.data import SentencepieceTokenizer \n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "from kogpt2.model.sample import sample_sequence\n",
        "from kogpt2.model.torch_gpt2 import GPT2Config, GPT2LMHeadModel\n",
        "from kogpt2.utils import download, get_tokenizer, tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnBdTTnyCiTL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pytorch_kogpt2 = {\n",
        "\t'url':\n",
        "\t'https://kobert.blob.core.windows.net/models/kogpt2/pytorch/pytorch_kogpt2_676e9bcfa7.params',\n",
        "\t'fname': 'pytorch_kogpt2_676e9bcfa7.params',\n",
        "\t'chksum': '676e9bcfa7'\n",
        "}\n",
        "\n",
        "kogpt2_config = {\n",
        "\t\"initializer_range\": 0.02,\n",
        "\t\"layer_norm_epsilon\": 1e-05,\n",
        "\t\"n_ctx\": 1024,\n",
        "\t\"n_embd\": 768,\n",
        "\t\"n_head\": 12,\n",
        "\t\"n_layer\": 12,\n",
        "\t\"n_positions\": 1024,\n",
        "\t\"vocab_size\": 50000,\n",
        "  \"output_past\": None\n",
        "}"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ht-bNABBqF_-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Read_Dataset(Dataset):\n",
        "\n",
        "\tdef __init__(self, file_path,vocab,tokenizer):\n",
        "\t\tself.file_path = file_path\n",
        "\t\tself.data =[]\n",
        "\t\tself.vocab =vocab\n",
        "\t\tself.tokenizer = tokenizer\n",
        "\t\tfile = open(self.file_path, 'r', encoding='utf-8')\n",
        "\n",
        "\t\tdf = pd.read_csv(self.file_path)\n",
        "\n",
        "\t\tdatasets = []\n",
        "\t\tfor _, row in df.iterrows():\n",
        "\t\t\tdatasets.append([row[\"제목\"]])\n",
        "\t\t\t\n",
        "\t\tprint(\"tokenizer ending\")\n",
        "\t\tfor line in datasets:\n",
        "\t\t\tif not line[0]:\n",
        "\t\t\t\tbreak\n",
        "\t\t\tif len(line[0]) > 1024:\n",
        "\t\t\t\tcontinue\n",
        "\t\t\telif len(line[0]) < 3:\n",
        "\t\t\t\tcontinue\n",
        "\t\t\ttokenized_line = tokenizer(line[0][:-1])\n",
        "\n",
        "\t\t\tindex_of_words = [vocab[vocab.bos_token], ] + vocab[tokenized_line] + [vocab[vocab.eos_token]]\n",
        "\n",
        "\t\t\tif len(index_of_words) < 3:\n",
        "\t\t\t\tcontinue\n",
        "\n",
        "\t\t\tself.data.append([index_of_words])\n",
        "\n",
        "\t\tprint(np.shape(self.data))\n",
        "\n",
        "\tdef __len__(self):\n",
        "\t\treturn len(self.data)\n",
        "\n",
        "\tdef __getitem__(self, index):\n",
        "\t\titem = self.data[index]\n",
        "\t\treturn item\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dBG5n5RX7aF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def auto_enter(text):\n",
        "\ttext = (text.replace(\"   \", \"\\n\"))\n",
        "\ttext = text.split(\"\\n\")\n",
        "\n",
        "\ttext = [t.lstrip() for t in text if t != '']\n",
        "\treturn \"\\n\\n\".join(text)\n",
        "\n",
        "\n",
        "def main(epoch, save_path, load_path, samples, data_file_path, batch_size):\n",
        "\tctx = 'cuda'\n",
        "\tcachedir = '~/kogpt2/'\n",
        "\n",
        "\tsummary = SummaryWriter()\n",
        "\n",
        "\tmodel_info = pytorch_kogpt2\n",
        "\tmodel_path = download(model_info['url'],\n",
        "\t\t\t\t\t\t   model_info['fname'],\n",
        "\t\t\t\t\t\t   model_info['chksum'],\n",
        "\t\t\t\t\t\t   cachedir=cachedir)\n",
        " \n",
        "\tvocab_info = tokenizer\n",
        "\tvocab_path = download(vocab_info['url'],\n",
        "\t\t\t\t\t\t   vocab_info['fname'],\n",
        "\t\t\t\t\t\t   vocab_info['chksum'],\n",
        "\t\t\t\t\t\t   cachedir=cachedir)\n",
        "\n",
        "\tkogpt2model = GPT2LMHeadModel(config=GPT2Config.from_dict(kogpt2_config))\n",
        "\n",
        "\tkogpt2model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "\tdevice = torch.device(ctx)\n",
        "\tkogpt2model.to(device)\n",
        "\n",
        "\ttry:\n",
        "\t\tcheckpoint = torch.load(load_path, map_location=device)\n",
        "\n",
        "\t\tkogpt2model = GPT2LMHeadModel(config=GPT2Config.from_dict(kogpt2_config))\n",
        "\t\tkogpt2model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "\t\tkogpt2model.eval()\n",
        "\texcept:\n",
        "\t\tcount = 0\n",
        "\telse:\n",
        "\t\tcount = int(re.findall(\"\\d+\", load_path)[2])\n",
        "\n",
        "\tprint(count)\n",
        " \n",
        "\tkogpt2model.train()\n",
        "\tvocab_b_obj = gluonnlp.vocab.BERTVocab.from_sentencepiece(vocab_path,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t mask_token=None,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t sep_token=None,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t cls_token=None,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t unknown_token='<unk>',\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t padding_token='<pad>',\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t bos_token='<s>',\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t eos_token='</s>')\n",
        "\n",
        "\n",
        "\ttok_path = get_tokenizer()\n",
        "\tmodel, vocab = kogpt2model, vocab_b_obj\n",
        "\ttok = SentencepieceTokenizer(tok_path)\n",
        "\n",
        "\tdataset = Read_Dataset(data_file_path, vocab, tok)\n",
        "\tprint(\"Read_Dataset ok\")\n",
        "\tdata_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
        "\n",
        "\n",
        "\n",
        "\tlearning_rate = 3e-5\n",
        "\tcriterion = torch.nn.CrossEntropyLoss()\n",
        "\toptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\tprint('KoGPT-2 Transfer Learning Start')\n",
        "\tavg_loss = (0.0, 0.0)\n",
        "\n",
        "\tfor epoch in range(epoch):\n",
        "\t\tfor data in data_loader:\n",
        "\t\t\toptimizer.zero_grad()\n",
        "\t\t\tdata = torch.stack(data[0])\n",
        "\t\t\tdata = data.transpose(1,0)\n",
        "\t\t\tdata = data.to(ctx)\n",
        "\t\t\tmodel = model.to(ctx)\n",
        "\n",
        "\t\t\toutputs = model(data, labels=data)\n",
        "\t\t\tloss, logits = outputs[:2]\n",
        "\t\t\tloss = loss.to(ctx)\n",
        "\t\t\tloss.backward()\n",
        "\t\t\tavg_loss = (avg_loss[0] * 0.99 + loss, avg_loss[1] * 0.99 + 1.0)\n",
        "\t\t\toptimizer.step()\n",
        "\t\t\tif count % 10000 == 0:\n",
        "\t\t\t\tprint('epoch no.{0} train no.{1}  loss = {2:.5f} avg_loss = {3:.5f}' . format(epoch, count, loss, avg_loss[0] / avg_loss[1]))\n",
        "\t\t\t\tsummary.add_scalar('loss/avg_loss', avg_loss[0] / avg_loss[1], count)\n",
        "\t\t\t\tsummary.add_scalar('loss/loss', loss, count)\n",
        "\n",
        "\n",
        "\t\t\tif (count > 0 and count % 10000 == 0) or (len(data) < batch_size):\n",
        "\t\t\t\tsent = sample_sequence(model.to(\"cpu\"), tok, vocab, sent=\"역량\", text_size=100, temperature=0.7, top_p=0.8, top_k=40)\n",
        "\t\t\t\tsent = sent.replace(\"<unused0>\", \"\\n\")\n",
        "\t\t\t\tsent = auto_enter(sent)\n",
        "\t\t\t\tprint(sent)\n",
        "\n",
        "\t\t\t\tsummary.add_text('Text', sent, count)\n",
        "\n",
        "\t\t\t\tif count > 100000:\n",
        "\t\t\t\t\tnow = [int(n) for n in os.listdir(samples)]\n",
        "\t\t\t\t\tnow = max(now)\n",
        "\t\t\t\t\tf = open(samples + str(now + 1), 'w', encoding=\"utf-8\")\n",
        "\t\t\t\t\tf.write(sent)\n",
        "\t\t\t\t\tf.close()\n",
        "\t\t \n",
        "\t\t\tcount += 1\n",
        "\n",
        "\t\t\tif (count > 0 and count % 10000 == 0) or (len(data) < batch_size):\n",
        "\t\t\t\t\n",
        "\t\t\t\ttry:\n",
        "\t\t\t\t\ttorch.save({\n",
        "\t\t\t\t\t\t'epoch': epoch,\n",
        "\t\t\t\t\t\t'train_no': count,\n",
        "\t\t\t\t\t\t'model_state_dict': model.state_dict(),\n",
        "\t\t\t\t\t\t'optimizer_state_dict': optimizer.state_dict(),\n",
        "\t\t\t\t\t\t'loss': loss\n",
        "\t\t\t\t\t}, save_path + 'KoGPT2_title_checkpoint_' + str(count) + '.tar')\n",
        "\t\t\t\texcept:\n",
        "\t\t\t\t\tpass"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VbGBKFnaDwd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir(\"/content/drive/My Drive/TMI\")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Phz88YPwbITI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from preprocess import _preprocess_qna\n",
        "\n",
        "df = pd.read_csv(\"jobkorea_all.csv\")\n",
        "df = _preprocess_qna(df)\n",
        "df = df.loc[:, \"제목\"]\n",
        "df.dropna().to_csv(\"dataset.txt\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M__ybw6R8Ud0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "c1599284-2180-4d8a-8641-ebac838f0cb6"
      },
      "source": [
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.device_count())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKOVsTtX6caR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "8fb53c93-a6e2-4277-c55a-14de776a55c2"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Aug 26 05:44:36 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.57       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   68C    P0    73W / 149W |    432MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXDB8a6pf3O0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "1877df18-e02c-4b30-de61-bbbac451064d"
      },
      "source": [
        "main(epoch=10,\n",
        "     save_path=\"/content/drive/My Drive/TMI/KoGPT2/\",\n",
        "     load_path=\"/content/drive/My Drive/TMI/KoGPT2/KoGPT2_title_checkpoint_30000.tar\",\n",
        "     samples=\"/content/drive/My Drive/TMI/KoGPT2/\",\n",
        "     data_file_path=\"dataset.txt\",\n",
        "     batch_size=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[██████████████████████████████████████████████████]\n",
            "[██████████████████████████████████████████████████]\n",
            "30000\n",
            "using cached model\n",
            "tokenizer ending\n",
            "(13528, 1)\n",
            "Read_Dataset ok\n",
            "KoGPT-2 Transfer Learning Start\n",
            "epoch no.0 train no.30000  loss = 4.41747 avg_loss = 4.41747\n",
            "2\n",
            "to_tokens: ['▁\"', '개발을', '▁엔', '</s>']\n",
            "역량 있는 인</s>\n",
            "epoch no.0 train no.40000  loss = 4.71435 avg_loss = 3.52046\n",
            "7\n",
            "to_tokens: ['▁\"', '개발을', '▁위한', '▁노', ',', '▁그리고', '▁성취', '</s>', '</s>']\n",
            "역량개발을 위한 노력, 그리고 성과달</s>\n",
            "epoch no.1 train no.50000  loss = 3.49887 avg_loss = 2.98632\n",
            "3\n",
            "to_tokens: ['▁\"', '개발을', '▁위한', '▁역량', '</s>']\n",
            "역량 강화를 위한 노</s>\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}