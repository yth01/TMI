{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KoGPT2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIwa4f0rU_TD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnAN6wQhV8Je",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "os.chdir(\"/content/drive/My Drive/TMI/KoGPT2\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgFiKU6VWGwX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dA3PGZ0TkUGy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "import re\n",
        "import subprocess\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import gluonnlp\n",
        "import torch\n",
        "from gluonnlp.data import SentencepieceTokenizer \n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "from kogpt2.model.sample import sample_sequence\n",
        "from kogpt2.model.torch_gpt2 import GPT2Config, GPT2LMHeadModel\n",
        "from kogpt2.utils import download, get_tokenizer, tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnBdTTnyCiTL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pytorch_kogpt2 = {\n",
        "\t'url':\n",
        "\t'https://kobert.blob.core.windows.net/models/kogpt2/pytorch/pytorch_kogpt2_676e9bcfa7.params',\n",
        "\t'fname': 'pytorch_kogpt2_676e9bcfa7.params',\n",
        "\t'chksum': '676e9bcfa7'\n",
        "}\n",
        "\n",
        "kogpt2_config = {\n",
        "\t\"initializer_range\": 0.02,\n",
        "\t\"layer_norm_epsilon\": 1e-05,\n",
        "\t\"n_ctx\": 1024,\n",
        "\t\"n_embd\": 768,\n",
        "\t\"n_head\": 12,\n",
        "\t\"n_layer\": 12,\n",
        "\t\"n_positions\": 1024,\n",
        "\t\"vocab_size\": 50000,\n",
        "  \"output_past\": None\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ht-bNABBqF_-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Read_Dataset(Dataset):\n",
        "\n",
        "\tdef __init__(self, file_path,vocab,tokenizer):\n",
        "\t\tself.file_path = file_path\n",
        "\t\tself.data =[]\n",
        "\t\tself.vocab =vocab\n",
        "\t\tself.tokenizer = tokenizer\n",
        "\t\tfile = open(self.file_path, 'r', encoding='utf-8')\n",
        "\n",
        "\t\tdf = pd.read_csv(self.file_path)\n",
        "\n",
        "\t\tdatasets = []\n",
        "\t\tfor _, row in df.iterrows():\n",
        "\t\t\tdatasets.append([row[\"답변\"]])\n",
        "\t\t\t\n",
        "\t\tprint(\"tokenizer ending\")\n",
        "\t\tfor line in datasets:\n",
        "\t\t\tif not line[0]:\n",
        "\t\t\t\tbreak\n",
        "\t\t\tif len(line[0]) < 3:\n",
        "\t\t\t\tcontinue\n",
        "\t\t\ttokenized_line = tokenizer(line[0][:-1])\n",
        "\n",
        "\t\t\tindex_of_words = [vocab[vocab.bos_token], ] + vocab[tokenized_line] + [vocab[vocab.eos_token]]\n",
        "\n",
        "\t\t\tif len(index_of_words) > 1024:\n",
        "\t\t\t\tcontinue\n",
        "\t\t\telif len(index_of_words) < 100:\n",
        "\t\t\t\tcontinue\n",
        "\n",
        "\t\t\tself.data.append([index_of_words])\n",
        "\n",
        "\t\tprint(np.shape(self.data))\n",
        "\n",
        "\tdef __len__(self):\n",
        "\t\treturn len(self.data)\n",
        "\n",
        "\tdef __getitem__(self, index):\n",
        "\t\titem = self.data[index]\n",
        "\t\treturn item"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dBG5n5RX7aF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def auto_enter(text):\n",
        "\ttext = (text.replace(\"   \", \"\\n\"))\n",
        "\ttext = text.split(\"\\n\")\n",
        "\n",
        "\ttext = [t.lstrip() for t in text if t != '']\n",
        "\treturn \"\\n\\n\".join(text)\n",
        "\n",
        "\n",
        "def main(epoch, save_path, load_path, samples, data_file_path, batch_size):\n",
        "\tctx = 'cuda'\n",
        "\tcachedir = '~/kogpt2/'\n",
        "\n",
        "\tsummary = SummaryWriter()\n",
        "\n",
        "\tmodel_info = pytorch_kogpt2\n",
        "\tmodel_path = download(model_info['url'],\n",
        "\t\t\t\t\t\t   model_info['fname'],\n",
        "\t\t\t\t\t\t   model_info['chksum'],\n",
        "\t\t\t\t\t\t   cachedir=cachedir)\n",
        " \n",
        "\tvocab_info = tokenizer\n",
        "\tvocab_path = download(vocab_info['url'],\n",
        "\t\t\t\t\t\t   vocab_info['fname'],\n",
        "\t\t\t\t\t\t   vocab_info['chksum'],\n",
        "\t\t\t\t\t\t   cachedir=cachedir)\n",
        "\n",
        "\tkogpt2model = GPT2LMHeadModel(config=GPT2Config.from_dict(kogpt2_config))\n",
        "\n",
        "\tkogpt2model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "\tdevice = torch.device(ctx)\n",
        "\tkogpt2model.to(device)\n",
        "\n",
        "\ttry:\n",
        "\t\tcheckpoint = torch.load(load_path, map_location=device)\n",
        "\n",
        "\t\tkogpt2model = GPT2LMHeadModel(config=GPT2Config.from_dict(kogpt2_config))\n",
        "\t\tkogpt2model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "\t\tkogpt2model.eval()\n",
        "\texcept:\n",
        "\t\tcount = 0\n",
        "\telse:\n",
        "\t\tcount = int(re.findall(\"\\d+\", load_path)[1])\n",
        "\n",
        "\tprint(count)\n",
        " \n",
        "\tkogpt2model.train()\n",
        "\tvocab_b_obj = gluonnlp.vocab.BERTVocab.from_sentencepiece(vocab_path,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t mask_token=None,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t sep_token=None,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t cls_token=None,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t unknown_token='<unk>',\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t padding_token='<pad>',\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t bos_token='<s>',\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t eos_token='</s>')\n",
        "\n",
        "\n",
        "\ttok_path = get_tokenizer()\n",
        "\tmodel, vocab = kogpt2model, vocab_b_obj\n",
        "\ttok = SentencepieceTokenizer(tok_path)\n",
        "\n",
        "\tdataset = Read_Dataset(data_file_path, vocab, tok)\n",
        "\tprint(\"Read_Dataset ok\")\n",
        "\tdata_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
        "\n",
        "\n",
        "\n",
        "\tlearning_rate = 3e-5\n",
        "\tcriterion = torch.nn.CrossEntropyLoss()\n",
        "\toptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\tprint('KoGPT-2 Transfer Learning Start')\n",
        "\tavg_loss = (0.0, 0.0)\n",
        "\n",
        "\tfor epoch in range(epoch):\n",
        "\t\tfor data in data_loader:\n",
        "\t\t\toptimizer.zero_grad()\n",
        "\t\t\tdata = torch.stack(data[0])\n",
        "\t\t\tdata = data.transpose(1,0)\n",
        "\t\t\tdata = data.to(ctx)\n",
        "\t\t\tmodel = model.to(ctx)\n",
        "\n",
        "\t\t\toutputs = model(data, labels=data)\n",
        "\t\t\tloss, logits = outputs[:2]\n",
        "\t\t\tloss = loss.to(ctx)\n",
        "\t\t\tloss.backward()\n",
        "\t\t\tavg_loss = (avg_loss[0] * 0.99 + loss, avg_loss[1] * 0.99 + 1.0)\n",
        "\t\t\toptimizer.step()\n",
        "\t\t\tif count % 100 == 0:\n",
        "\t\t\t\tprint('epoch no.{0} train no.{1}  loss = {2:.5f} avg_loss = {3:.5f}' . format(epoch, count, loss, avg_loss[0] / avg_loss[1]))\n",
        "\t\t\t\tsummary.add_scalar('loss/avg_loss', avg_loss[0] / avg_loss[1], count)\n",
        "\t\t\t\tsummary.add_scalar('loss/loss', loss, count)\n",
        "\n",
        "\n",
        "\t\t\tif (count > 0 and count % 1000 == 0) or (len(data) < batch_size):\n",
        "\t\t\t\tsent = sample_sequence(model.to(\"cpu\"), tok, vocab, sent=\"지원\", text_size=100, temperature=0.7, top_p=0.8, top_k=40)\n",
        "\t\t\t\tsent = sent.replace(\"<unused0>\", \"\\n\")\n",
        "\t\t\t\tsent = auto_enter(sent)\n",
        "\t\t\t\tprint(sent)\n",
        "\n",
        "\t\t\t\tsummary.add_text('Text', sent, count)\n",
        "\n",
        "\t\t\t\tif count > 500000:\n",
        "\t\t\t\t\tnow = [int(n) for n in os.listdir(samples)]\n",
        "\t\t\t\t\tnow = max(now)\n",
        "\t\t\t\t\tf = open(samples + str(now + 1), 'w', encoding=\"utf-8\")\n",
        "\t\t\t\t\tf.write(sent)\n",
        "\t\t\t\t\tf.close()\n",
        "\t\t \n",
        "\t\t\tcount += 1\n",
        "\n",
        "\t\t\tif (count > 0 and count % 10000 == 0) or (len(data) < batch_size):\n",
        "\t\t\t\t\n",
        "\t\t\t\ttry:\n",
        "\t\t\t\t\ttorch.save({\n",
        "\t\t\t\t\t\t'epoch': epoch,\n",
        "\t\t\t\t\t\t'train_no': count,\n",
        "\t\t\t\t\t\t'model_state_dict': model.state_dict(),\n",
        "\t\t\t\t\t\t'optimizer_state_dict': optimizer.state_dict(),\n",
        "\t\t\t\t\t\t'loss': loss\n",
        "\t\t\t\t\t}, save_path + 'KoGPT2_checkpoint_' + str(count) + '.tar')\n",
        "\t\t\t\texcept:\n",
        "\t\t\t\t\tpass"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VbGBKFnaDwd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir(\"/content/drive/My Drive/TMI\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Phz88YPwbITI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from preprocess import _preprocess_qna\n",
        "\n",
        "df = pd.read_csv(\"jobkorea_all.csv\")\n",
        "df = _preprocess_qna(df)\n",
        "df = df[[\"질문\", \"답변\", \"총평\"]]\n",
        "df.loc[:, \"답변\"].to_csv(\"dataset.txt\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M__ybw6R8Ud0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "b58ab1f0-8784-41d7-cce4-e8f05a2e9cf8"
      },
      "source": [
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.device_count())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKOVsTtX6caR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "f350eedb-4ba8-493e-b6c6-ca822fe36daf"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Aug 24 15:12:59 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.57       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P0    58W / 149W |    432MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXDB8a6pf3O0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "56fd0086-adf4-4d3e-d07b-f51e5a1f6a6a"
      },
      "source": [
        "main(epoch=1,\n",
        "     save_path=\"/content/drive/My Drive/TMI/KoGPT2/\",\n",
        "     load_path=None,\n",
        "     samples=\"/content/drive/My Drive/TMI/KoGPT2/\",\n",
        "     data_file_path=\"dataset.txt\",\n",
        "     batch_size=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[██████████████████████████████████████████████████]\n",
            "[██████████████████████████████████████████████████]\n",
            "0\n",
            "using cached model\n",
            "tokenizer ending\n",
            "(23335, 1)\n",
            "Read_Dataset ok\n",
            "KoGPT-2 Transfer Learning Start\n",
            "epoch no.0 train no.0  loss = 5.63261 avg_loss = 5.63261\n",
            "epoch no.0 train no.100  loss = 5.16856 avg_loss = 4.78414\n",
            "epoch no.0 train no.200  loss = 4.04943 avg_loss = 4.67989\n",
            "epoch no.0 train no.300  loss = 4.85964 avg_loss = 4.67364\n",
            "epoch no.0 train no.400  loss = 4.88377 avg_loss = 4.64908\n",
            "epoch no.0 train no.500  loss = 4.78443 avg_loss = 4.61110\n",
            "epoch no.0 train no.600  loss = 4.92731 avg_loss = 4.62429\n",
            "epoch no.0 train no.700  loss = 4.31770 avg_loss = 4.56981\n",
            "epoch no.0 train no.800  loss = 4.55606 avg_loss = 4.56274\n",
            "epoch no.0 train no.900  loss = 4.79640 avg_loss = 4.53951\n",
            "epoch no.0 train no.1000  loss = 4.22772 avg_loss = 4.50387\n",
            "101\n",
            "to_tokens: ['▁저는', '자의', '▁지원', '학기', '▁이상의', '▁전공', '에', '▁지원', '▁다양한', '에', '▁대해', '▁전문성을', '한', '▁후', '▁지원', '개', '▁이상의', '▁직무', '에', '▁걸쳐', '▁공부', '한', '▁후', '▁2', '에', '▁따라', '▁공부', '한', '▁후', '▁직무', '개', '▁이상의', '▁직무', '에', '▁대해', '▁공부', '한', '▁후', '▁직무', '개', '▁이상의', '▁직무', '에', '▁대해', '▁공부', '한', '▁후', '▁직무', '에', '▁대해', '▁공부', '한', '▁후', '▁직무', '개', '▁이상의', '▁직무', '에', '▁대해', '▁공부', '한', '▁후', '▁직무', '개', '▁이상의', '▁직무', '에', '▁대해', '▁공부', '한', '▁후', '▁직무', '에', '▁대해', '▁공부', '한', '▁후', '▁직무', '에', '▁대해', '▁공부', '한', '▁후', '▁직무', '에', '▁대해', '▁공부', '한', '▁후', '▁직무', '에', '▁대해', '▁공부', '한', '▁후', '▁직무', '에', '▁대해', '▁공부', '한', '▁후', '▁직무', '에']\n",
            "지원자는 2개 이상의 직무에 걸쳐 직무에 대해 공부한 후 2개 이상의 직무에 대해 공부한 후 직무에 대해 공부한 후 2개 이상의 직무에 대해 공부한 후 2개 이상의 직무에 대해 공부한 후 직무에 대해 공부한 후 2개 이상의 직무에 대해 공부한 후 2개 이상의 직무에 대해 공부한 후 직무에 대해 공부한 후 직무에 대해 공부한 후 직무에 대해 공부한 후 직무에 대해 공부한 후 직무에 대해 공부한 후 직무에\n",
            "epoch no.0 train no.1100  loss = 5.05783 avg_loss = 4.46853\n",
            "epoch no.0 train no.1200  loss = 4.36868 avg_loss = 4.47096\n",
            "epoch no.0 train no.1300  loss = 4.69578 avg_loss = 4.45488\n",
            "epoch no.0 train no.1400  loss = 4.35722 avg_loss = 4.44322\n",
            "epoch no.0 train no.1500  loss = 4.61617 avg_loss = 4.43483\n",
            "epoch no.0 train no.1600  loss = 4.62796 avg_loss = 4.42099\n",
            "epoch no.0 train no.1700  loss = 4.37877 avg_loss = 4.42188\n",
            "epoch no.0 train no.1800  loss = 4.24217 avg_loss = 4.43593\n",
            "epoch no.0 train no.1900  loss = 4.31425 avg_loss = 4.41708\n",
            "epoch no.0 train no.2000  loss = 4.55424 avg_loss = 4.41870\n",
            "101\n",
            "to_tokens: ['▁제가', '자가', '▁되기', '▁싶은', '▁하는', '▁이유는', '▁가장', '▁중요하다고', '▁떠오르는', '▁것은', '입니다', '▁저는', '▁지원', '▁시절', '학년', '▁때', '부터', '▁지금까지', '▁다양한', '▁2', '학년', '때', '▁까지', '▁번도', '기를', '▁한', '▁번', '은', '▁꼭', '공', '부를', '▁했습니다', '▁하지만', '▁3', '학년', '▁때', '▁교내', '▁속한', '▁학과', '에서', '▁한', '▁학기', '▁동안', '▁한', '공', '▁듣', '▁적이', '▁있습니다', '▁저는', '▁전공', '▁듣는', '▁수업을', '▁듣는', '▁적이', '▁수업을', '을', '서를', '▁수강', '▁수행을', '▁등', '▁통해', '▁그', '▁수업', '은', '▁수업', '▁영어로', '▁후', '였습니다', '▁저는', '은', '▁대한', '▁학생들은', '▁수업', '에', '▁대한', '▁학생들은', '▁중', '▁수업', '에', '▁대한', '했습니다', '▁원', '했고', '▁저는', '▁수업', '에', '▁참여하지', '▁않았습니다', '▁수업', '▁수업', '신청', '과', '▁과제', '수행', '▁등을', '▁통해', '▁수업', '▁듣고', '▁수업을', '▁들은', '▁후', '▁수강', '신청을']\n",
            "지원자가 되고자 하는 마음이 가장 먼저 떠오르는 사람입니다 저는 고등학교 1학년때부터 지금까지 대학교 3학년 때 한 학기에 한 번씩은 학과공부를 했습니다 대학교 3학년 때는 제가 속한 과에서 한 학기 동안 학부 수업을 들은 적이 있습니다 저는 수업을 듣고 수업을 들은 후 수강신청과 과제수행 등을 했습니다 하지만 수업은 모두 끝난 뒤였습니다 수업에 참여한 후 수업에 참여한 학생 모두가 수업에 참여하기를 원했지만 저는 수업에 참여하지 않았습니다 저는 수강신청과 과제수행 등을 통해 수업을 듣고 수업을 들은 후 수강신청을\n",
            "epoch no.0 train no.2100  loss = 4.95276 avg_loss = 4.41077\n",
            "epoch no.0 train no.2200  loss = 4.15190 avg_loss = 4.39546\n",
            "epoch no.0 train no.2300  loss = 4.62999 avg_loss = 4.39857\n",
            "epoch no.0 train no.2400  loss = 4.36865 avg_loss = 4.40956\n",
            "epoch no.0 train no.2500  loss = 4.50716 avg_loss = 4.39267\n",
            "epoch no.0 train no.2600  loss = 4.83787 avg_loss = 4.37242\n",
            "epoch no.0 train no.2700  loss = 3.94660 avg_loss = 4.36855\n",
            "epoch no.0 train no.2800  loss = 4.45482 avg_loss = 4.37145\n",
            "epoch no.0 train no.2900  loss = 4.55121 avg_loss = 4.39235\n",
            "epoch no.0 train no.3000  loss = 4.21267 avg_loss = 4.40538\n",
            "101\n",
            "to_tokens: ['▁저는', '자가', '과', '에서', '▁영어', '회화', '▁수업', '사를', '▁맡', '았습니다', '▁영어', '▁영어', '▁수업', '▁중', '▁영어', '회화', '▁강', '▁맡', '었고', '▁영어', '▁것을', '▁보냈', '습니다', '▁영어', '▁중', '▁영어', '▁된', '▁수업', '은', '▁익숙', '하지', '▁않아', '▁저는', '▁영어', '회화', '▁수업', '에', '▁익숙', '하지', '수록', '▁영어', '회화', '▁대한', '▁두려', '움이', '▁커', '▁커', '졌습니다', '▁하지만', '회화', '▁수업', '가', '▁익숙', '해', '지면', '▁영어', '회화', '▁대한', '▁두려', '움이', '▁더', '지고', '▁것', '▁또한', '▁영어', '회화', '▁강의', '가', '▁함께', '▁영어', '▁맡은', '▁수업', '회화', '▁수업', '에', '▁대한', '▁자신감이', '도', '▁사라', '질', '▁것입니다', '▁저는', '▁영어', '회화', '▁강의', '에', '▁더불어', '▁제가', '회화', '▁수업', '에', '▁배운', '▁영어', '회화', '▁실', '력에', '▁통해', '▁영어', '실', '▁대한', '▁두려', '움을', '▁극복']\n",
            "지원 전공수업 중 영어회화 강사를 맡았습니다 저는 전공수업 중 영어회화 강의를 들으면서 많은 시간을 보냈습니다 그 동안 영어로 배우는 수업에 익숙하지 않았던 저는 영어회화 강의가 익숙해질수록 영어에 대한 두려움이 더욱 커졌습니다 영어회화 강의가 익숙해지면 영어에 대한 두려움은 없어질 것입니다 또한 영어회화 강의와 더불어 제가 맡은 영어회화 수업에 대한 두려움도 없어질 것입니다 저는 영어회화 강의와 더불어 영어회화 수업에서 배운 영어회화 실기를 통해 영어에 대한 두려움을 극복\n",
            "epoch no.0 train no.3100  loss = 4.58216 avg_loss = 4.38314\n",
            "epoch no.0 train no.3200  loss = 4.36817 avg_loss = 4.38142\n",
            "epoch no.0 train no.3300  loss = 5.04113 avg_loss = 4.34065\n",
            "epoch no.0 train no.3400  loss = 3.81460 avg_loss = 4.34158\n",
            "epoch no.0 train no.3500  loss = 4.28951 avg_loss = 4.36067\n",
            "epoch no.0 train no.3600  loss = 4.80776 avg_loss = 4.36233\n",
            "epoch no.0 train no.3700  loss = 3.82265 avg_loss = 4.32679\n",
            "epoch no.0 train no.3800  loss = 4.27527 avg_loss = 4.33658\n",
            "epoch no.0 train no.3900  loss = 4.11889 avg_loss = 4.33581\n",
            "epoch no.0 train no.4000  loss = 4.78621 avg_loss = 4.36172\n",
            "101\n",
            "to_tokens: ['▁저는', '동기', '▁역량', '▁생각해', '보고', '▁저는', '▁같습니다', '▁저는', '▁고등학교', '▁2', '▁다양한', '▁때까지', '학년', '▁때', '▁고등학교', '▁2', '학년', '▁때까지', '▁다양한', '▁경험을', '▁했습니다', '왔습니다', '▁고등학교', '▁때부터', '▁고등학교', '까지', '▁다양한', '▁경험을', '들을', '▁해', '▁다양한', '▁경험을', '▁쌓', '▁수', '▁있었습니다', '▁고등학교', '▁2', '▁다양한', '▁번도', '▁동안', '▁다양한', '생', '등을', '▁한', '도', '에서도', '▁장학금을', '▁받을', '▁등', '▁다양한', '▁경험을', '▁해', '왔습니다', '▁이러한', '▁고등학교', '▁진학', '▁후에도', '부터는', '▁대학교', '▁2', '▁다양한', '▁경험을', '▁해', '왔습니다', '▁다양한', '▁경험을', '▁할', '▁다양한', '▁경험을', '▁할', '▁수', '▁있었습니다', '▁이러한', '▁경험을', '들을', '▁통해', '▁다양한', '▁가진', '▁다양한', '▁더', '웠고', '▁더', '▁자신을', '▁발전', '시키고', '▁발전', '▁것을', '▁만들어', '▁많이', '시킬', '▁인재', '▁저의', '▁목표', '입니다', '▁또한', '▁이러한', '▁경험을', '▁통해', '▁다양한', '▁지식을', '▁지식을', '▁바탕으로', '▁다양한']\n",
            "지원자의 입장에서 생각해보면 다음과 같습니다 저는 중학교 때부터 고등학교 2학년 때부터 고등학교 3학년 때까지 다양한 경험을 해왔습니다 초등학교부터 고등학교까지 다양한 경험들을 통해 다양한 경험을 할 수 있었고 고등학교에서 한 학기동안 전교 1등을 하여 대학교에서도 장학금을 받는 등 다양한 경험을 해왔습니다 또한 대학 입학 후부터 대학교까지 다양한 경험을 해오면서 다양한 경험을 통해 다양한 경험을 할 수 있었습니다 이러한 경험들을 통해 제가 가진 것들을 배워서 제 자신을 발전시키고 새로운 것을 더 발전시키는 것이 제 목표입니다 또한 다양한 경험을 통해 얻은 경험과 경험을 통해 제가\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}